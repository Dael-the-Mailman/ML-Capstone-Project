{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eef90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cba07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=72, input_size=1326, dropout=0.2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Encoder Layers\n",
    "        self.enc1 = nn.Linear(input_size,1024)\n",
    "        self.enc2 = nn.Linear(1024,512)\n",
    "        self.enc3 = nn.Linear(512,256)\n",
    "        self.enc4 = nn.Linear(256,128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, latent_dim)\n",
    "        self.fc_mu = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, 128)\n",
    "        \n",
    "        # Decoder Layers\n",
    "        self.dec1 = nn.Linear(128,256)\n",
    "        self.dec2 = nn.Linear(256,512)\n",
    "        self.dec3 = nn.Linear(512,1024)\n",
    "        self.dec4 = nn.Linear(1024,input_size)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + (eps * std)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.enc1(x)), p=self.dropout)\n",
    "        x = F.dropout(F.relu(self.enc2(x)), p=self.dropout)\n",
    "        x = F.dropout(F.relu(self.enc3(x)), p=self.dropout)\n",
    "        x = F.dropout(F.relu(self.enc4(x)), p=self.dropout)\n",
    "        hidden = self.fc1(x)\n",
    "        \n",
    "        mu = self.fc_mu(hidden)\n",
    "        log_var = self.fc_log_var(hidden)\n",
    "        \n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        z = self.fc2(z)\n",
    "        \n",
    "        x = F.dropout(F.relu(self.dec1(z)), p=self.dropout)\n",
    "        x = F.dropout(F.relu(self.dec2(x)), p=self.dropout)\n",
    "        x = F.dropout(F.relu(self.dec3(x)), p=self.dropout)\n",
    "        reconstruction = self.dec4(x)\n",
    "        \n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c63fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    BCE = bce_loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7818abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values('../.env')\n",
    "FEATURES = [str(c) for c in range(1326)]\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "model = VAE().to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f52fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(reconstruction, data) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     16\u001b[0m        F\u001b[38;5;241m.\u001b[39mkl_div(reconstruction, data, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(running_loss)\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for chunk in tqdm(pd.read_csv(config[\"ENGINEERED_DATA\"] + \"imputed_train.csv\",\n",
    "                             chunksize=batch_size, usecols=FEATURES)):\n",
    "        counter += 1\n",
    "        data = torch.Tensor(chunk.values)\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        loss = F.binary_cross_entropy(reconstruction, data) + \\\n",
    "               F.kl_div(reconstruction, data, reduction='batchmean')\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        print(running_loss)\n",
    "        optimizer.step()\n",
    "    print(f\"Loss: {running_loss/counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbadc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
