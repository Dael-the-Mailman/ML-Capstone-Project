{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a86196",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1247d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "from network import TabNet\n",
    "from dotenv import dotenv_values\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8702d9a",
   "metadata": {},
   "source": [
    "# Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"../.env\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using\", device, \"device\")\n",
    "BATCH_SIZE = 1024\n",
    "PATIENCE = 3 # How many epochs will we wait until performance gets better or not?\n",
    "SAVE_PATH = \"./model_parameters/\"\n",
    "TIMEOUT = 12*60*60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4168f",
   "metadata": {},
   "source": [
    "# Metric Used For Validation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd190d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric_mod(y_true, y_pred):\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620c24d",
   "metadata": {},
   "source": [
    "# Optuna Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Model Hyperparameters\n",
    "    param = {\n",
    "        \"input_dim\": 2319,\n",
    "        \"output_dim\": 1,\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 4, 64),\n",
    "        \"n_a\": trial.suggest_int(\"n_a\", 4, 64),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0, log=True),\n",
    "        \"cat_idxs\": [],\n",
    "        \"cat_dims\": [],\n",
    "        \"cat_emb_dim\": 1,\n",
    "        \"n_independent\": 2,\n",
    "        \"n_shared\": 2,\n",
    "        \"epsilon\": 1e-15,\n",
    "        \"vbs\": 128,\n",
    "        \"momentum\": trial.suggest_float(\"momentum\", 0.02, 1.0, log=True)\n",
    "    }\n",
    "    model = TabNet(**param).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "\n",
    "    first_pass = True\n",
    "    oof_tensors = {}\n",
    "    best_metric = 0.0 # Keeps track of best metric performance\n",
    "    patience_count = 0\n",
    "    for epoch in range(1,101): # Runs maximum of 100 epochs\n",
    "        # Load Data\n",
    "        labels = pd.read_csv(config[\"TRAIN_LABELS_PATH\"], \n",
    "                             chunksize=BATCH_SIZE)\n",
    "        df = pd.read_csv(config[\"WRANGLED_DATA\"] + \"scaled_train/train-0.csv.part\", \n",
    "                         chunksize=BATCH_SIZE)\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (chunk, chunk_labels) in enumerate(zip(df, labels)):\n",
    "            random = np.random.randint(5) # Determines which entries are going to be used in oof prediction\n",
    "            x = torch.Tensor(chunk.values).to(device, non_blocking=True)\n",
    "            y = torch.Tensor(chunk_labels[\"target\"].values).reshape(-1, 1).to(device, non_blocking=True)\n",
    "            if random == 0 and first_pass:\n",
    "                # If it's the first pass create the validation set\n",
    "                oof_tensors[i] = (x, y)\n",
    "                continue\n",
    "            if not first_pass and i in oof_tensors.keys():\n",
    "                # If not the first pass then skip the training on current entry\n",
    "                continue\n",
    "\n",
    "            # Train Model\n",
    "            y_hat, M_loss = model(x)\n",
    "            loss = F.mse_loss(y_hat, y) - (1e-3*M_loss)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 1) # Clip gradient\n",
    "            optimizer.step()\n",
    "            total_loss += loss.cpu().detach().numpy().item()\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        # Validate \n",
    "        model.eval()\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for x, y in list(oof_tensors.values()):\n",
    "            y_hat, _ = model(x)\n",
    "            preds += y_hat.cpu().detach().numpy().flatten().tolist()\n",
    "            labels += y.cpu().detach().numpy().flatten().tolist()\n",
    "        metric = amex_metric_mod(labels, preds)\n",
    "        print(f\"Epoch {epoch} | train_loss: {total_loss / (i-len(oof_tensors)+1):.4f} | validation_metric: {metric:.4f}\")\n",
    "        first_pass = False\n",
    "\n",
    "        # Saves model based on performance over time\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            patience_count = 0\n",
    "            id = datetime.now().strftime(\"%d-%m-%Y-%H%M%S\")\n",
    "            torch.save(model.state_dict(), SAVE_PATH+f\"Optuna_TabNet_{best_metric:.4f}_{id}.pt\")\n",
    "        else:\n",
    "            patience_count += 1\n",
    "        \n",
    "        # If model hasn't improved in given time training stops\n",
    "        if patience_count >= PATIENCE:\n",
    "            print(\"Early Stopping Activated!!!\")\n",
    "            break\n",
    "    \n",
    "    return best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, timeout=TIMEOUT)\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
