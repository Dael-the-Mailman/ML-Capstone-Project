{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36dc1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a878e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd1b9a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>P_2_first</th>\n",
       "      <th>P_2_mean</th>\n",
       "      <th>P_2_std</th>\n",
       "      <th>P_2_min</th>\n",
       "      <th>P_2_max</th>\n",
       "      <th>P_2_last</th>\n",
       "      <th>D_39_first</th>\n",
       "      <th>D_39_mean</th>\n",
       "      <th>D_39_std</th>\n",
       "      <th>...</th>\n",
       "      <th>D_64_nunique</th>\n",
       "      <th>D_66_count</th>\n",
       "      <th>D_66_first</th>\n",
       "      <th>D_66_last</th>\n",
       "      <th>D_66_nunique</th>\n",
       "      <th>D_68_count</th>\n",
       "      <th>D_68_first</th>\n",
       "      <th>D_68_last</th>\n",
       "      <th>D_68_nunique</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.938469</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.868580</td>\n",
       "      <td>0.960384</td>\n",
       "      <td>0.934745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.832050</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>0.929122</td>\n",
       "      <td>0.899820</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>0.861109</td>\n",
       "      <td>0.929122</td>\n",
       "      <td>0.880519</td>\n",
       "      <td>13</td>\n",
       "      <td>7.153846</td>\n",
       "      <td>6.743468</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>0.876615</td>\n",
       "      <td>0.878454</td>\n",
       "      <td>0.028911</td>\n",
       "      <td>0.797670</td>\n",
       "      <td>0.904482</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>0.567442</td>\n",
       "      <td>0.598969</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.567442</td>\n",
       "      <td>0.623392</td>\n",
       "      <td>0.621776</td>\n",
       "      <td>9</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>3.017046</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>0.042325</td>\n",
       "      <td>0.805045</td>\n",
       "      <td>0.940382</td>\n",
       "      <td>0.871900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1462 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  P_2_first  P_2_mean  \\\n",
       "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...   0.938469  0.933824   \n",
       "1  00000fd6641609c6ece5454664794f0340ad84dddce9a2...   0.929122  0.899820   \n",
       "2  00001b22f846c82c51f6e3958ccd81970162bae8b007e8...   0.876615  0.878454   \n",
       "3  000041bdba6ecadd89a52d11886e8eaaec9325906c9723...   0.567442  0.598969   \n",
       "4  00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...   0.936842  0.891679   \n",
       "\n",
       "    P_2_std   P_2_min   P_2_max  P_2_last  D_39_first  D_39_mean  D_39_std  \\\n",
       "0  0.024194  0.868580  0.960384  0.934745           0   0.230769  0.832050   \n",
       "1  0.022119  0.861109  0.929122  0.880519          13   7.153846  6.743468   \n",
       "2  0.028911  0.797670  0.904482  0.880875           0   0.000000  0.000000   \n",
       "3  0.020107  0.567442  0.623392  0.621776           9   1.538462  3.017046   \n",
       "4  0.042325  0.805045  0.940382  0.871900           0   0.000000  0.000000   \n",
       "\n",
       "   ...  D_64_nunique  D_66_count  D_66_first  D_66_last  D_66_nunique  \\\n",
       "0  ...             1          13          -1         -1             1   \n",
       "1  ...             1          13          -1         -1             1   \n",
       "2  ...             1          13          -1         -1             1   \n",
       "3  ...             1          13          -1         -1             1   \n",
       "4  ...             1          13           1          1             1   \n",
       "\n",
       "   D_68_count  D_68_first  D_68_last  D_68_nunique  target  \n",
       "0          13           6          6             1       0  \n",
       "1          13           6          6             1       0  \n",
       "2          13           6          6             1       0  \n",
       "3          13           2          3             3       0  \n",
       "4          13           6          6             1       0  \n",
       "\n",
       "[5 rows x 1462 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_parquet(config[\"ENGINEERED_DATA\"] + \"train_fe.parquet\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26ac7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 1462)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526c6b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_2_first</th>\n",
       "      <th>P_2_mean</th>\n",
       "      <th>P_2_std</th>\n",
       "      <th>P_2_min</th>\n",
       "      <th>P_2_max</th>\n",
       "      <th>P_2_last</th>\n",
       "      <th>D_39_first</th>\n",
       "      <th>D_39_mean</th>\n",
       "      <th>D_39_std</th>\n",
       "      <th>D_39_min</th>\n",
       "      <th>...</th>\n",
       "      <th>D_64_nunique</th>\n",
       "      <th>D_66_count</th>\n",
       "      <th>D_66_first</th>\n",
       "      <th>D_66_last</th>\n",
       "      <th>D_66_nunique</th>\n",
       "      <th>D_68_count</th>\n",
       "      <th>D_68_first</th>\n",
       "      <th>D_68_last</th>\n",
       "      <th>D_68_nunique</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938469</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.868580</td>\n",
       "      <td>0.960384</td>\n",
       "      <td>0.934745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.832050</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.929122</td>\n",
       "      <td>0.899820</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>0.861109</td>\n",
       "      <td>0.929122</td>\n",
       "      <td>0.880519</td>\n",
       "      <td>13</td>\n",
       "      <td>7.153846</td>\n",
       "      <td>6.743468</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.876615</td>\n",
       "      <td>0.878454</td>\n",
       "      <td>0.028911</td>\n",
       "      <td>0.797670</td>\n",
       "      <td>0.904482</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.567442</td>\n",
       "      <td>0.598969</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.567442</td>\n",
       "      <td>0.623392</td>\n",
       "      <td>0.621776</td>\n",
       "      <td>9</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>3.017046</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>0.042325</td>\n",
       "      <td>0.805045</td>\n",
       "      <td>0.940382</td>\n",
       "      <td>0.871900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1461 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   P_2_first  P_2_mean   P_2_std   P_2_min   P_2_max  P_2_last  D_39_first  \\\n",
       "0   0.938469  0.933824  0.024194  0.868580  0.960384  0.934745           0   \n",
       "1   0.929122  0.899820  0.022119  0.861109  0.929122  0.880519          13   \n",
       "2   0.876615  0.878454  0.028911  0.797670  0.904482  0.880875           0   \n",
       "3   0.567442  0.598969  0.020107  0.567442  0.623392  0.621776           9   \n",
       "4   0.936842  0.891679  0.042325  0.805045  0.940382  0.871900           0   \n",
       "\n",
       "   D_39_mean  D_39_std  D_39_min  ...  D_64_nunique  D_66_count  D_66_first  \\\n",
       "0   0.230769  0.832050         0  ...             1          13          -1   \n",
       "1   7.153846  6.743468         0  ...             1          13          -1   \n",
       "2   0.000000  0.000000         0  ...             1          13          -1   \n",
       "3   1.538462  3.017046         0  ...             1          13          -1   \n",
       "4   0.000000  0.000000         0  ...             1          13           1   \n",
       "\n",
       "   D_66_last  D_66_nunique  D_68_count  D_68_first  D_68_last  D_68_nunique  \\\n",
       "0         -1             1          13           6          6             1   \n",
       "1         -1             1          13           6          6             1   \n",
       "2         -1             1          13           6          6             1   \n",
       "3         -1             1          13           2          3             3   \n",
       "4          1             1          13           6          6             1   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 1461 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop(\"customer_ID\", axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6674cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.groupby('target').apply(pd.DataFrame.sample, frac=0.2).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe6ccde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91783, 1461)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96fad390",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'target'\n",
    "eval_metric='log_loss'\n",
    "save_path=config[\"AG_MODELS\"]+\"model_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d25dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"E:/datasets/amex-default-prediction/models/ag_models/model_4\"\n",
      "Presets specified: ['best_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'ag_args_fit': {'num_gpus': 1}, 'auto_stack': True}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': {'num_gpus': 1},\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'quantile_levels': None,\n",
      " 'refit_full': False,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\learner.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"E:/datasets/amex-default-prediction/models/ag_models/model_4\\\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.9.12\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    91783\n",
      "Train Data Columns: 1460\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5101.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 553.18 MB (10.8% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 10.8% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 91 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float32', 'float') : 651 | ['P_2_first', 'P_2_mean', 'P_2_min', 'P_2_max', 'P_2_last', ...]\n",
      "\t\t\t\t('float64', 'float') : 345 | ['P_2_std', 'D_39_mean', 'D_39_std', 'B_1_std', 'B_2_std', ...]\n",
      "\t\t\t\t('int16', 'int')     :  45 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'B_4_first', ...]\n",
      "\t\t\t\t('int64', 'int')     :  22 | ['B_30_count', 'B_30_nunique', 'B_38_count', 'B_38_nunique', 'D_114_count', ...]\n",
      "\t\t\t\t('int8', 'int')      : 396 | ['D_44_first', 'D_44_min', 'D_44_max', 'D_44_last', 'R_2_first', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 996 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t\t\t('int', [])   : 463 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 995 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t\t\t('int', [])       : 373 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t\t\t('int', ['bool']) :  91 | ['R_2_first', 'R_2_min', 'R_2_max', 'R_2_last', 'S_6_first', ...]\n",
      "\t\t\t3.4s = Fit runtime\n",
      "\t\t\t1459 features in original data used to generate 1459 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 995 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t\t\t('int', [])       : 373 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t\t\t('int', ['bool']) :  91 | ['R_2_first', 'R_2_min', 'R_2_max', 'R_2_last', 'S_6_first', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 995 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t\t\t('int', [])       : 373 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t\t\t('int', ['bool']) :  91 | ['R_2_first', 'R_2_min', 'R_2_max', 'R_2_last', 'S_6_first', ...]\n",
      "\t\t\t0.2s = Fit runtime\n",
      "\t\t\t1459 features in original data used to generate 1459 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 995 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t\t\t('int', [])       : 373 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t\t\t('int', ['bool']) :  91 | ['R_2_first', 'R_2_min', 'R_2_max', 'R_2_last', 'S_6_first', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 995 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t\t\t('int', [])       : 373 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t\t\t('int', ['bool']) :  91 | ['R_2_first', 'R_2_min', 'R_2_max', 'R_2_last', 'S_6_first', ...]\n",
      "\t\t\t0.2s = Fit runtime\n",
      "\t\t\t1459 features in original data used to generate 1459 features in processed data.\n",
      "\t\tSkipping CategoryFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 995 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t\t\t('int', [])       : 373 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t\t\t('int', ['bool']) :  91 | ['R_2_first', 'R_2_min', 'R_2_max', 'R_2_last', 'S_6_first', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 995 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t\t\t('int', [])       : 373 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t\t\t('int', ['bool']) :  91 | ['R_2_first', 'R_2_min', 'R_2_max', 'R_2_last', 'S_6_first', ...]\n",
      "\t\t\t2.2s = Fit runtime\n",
      "\t\t\t1459 features in original data used to generate 1459 features in processed data.\n",
      "\tUseless Original Features (Count: 1): ['R_23_min']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float32', 'float') : 651 | ['P_2_first', 'P_2_mean', 'P_2_min', 'P_2_max', 'P_2_last', ...]\n",
      "\t\t('float64', 'float') : 345 | ['P_2_std', 'D_39_mean', 'D_39_std', 'B_1_std', 'B_2_std', ...]\n",
      "\t\t('int16', 'int')     :  45 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'B_4_first', ...]\n",
      "\t\t('int64', 'int')     :  22 | ['B_30_count', 'B_30_nunique', 'B_38_count', 'B_38_nunique', 'D_114_count', ...]\n",
      "\t\t('int8', 'int')      : 396 | ['D_44_first', 'D_44_min', 'D_44_max', 'D_44_last', 'R_2_first', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 996 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t('int', [])   : 463 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('float32', 'float') : 651 | ['P_2_first', 'P_2_mean', 'P_2_min', 'P_2_max', 'P_2_last', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t('float64', 'float') : 344 | ['P_2_std', 'D_39_mean', 'D_39_std', 'B_1_std', 'B_2_std', ...]\n",
      "\t\t('int16', 'int')     :  45 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'B_4_first', ...]\n",
      "\t\t('int64', 'int')     :  22 | ['B_30_count', 'B_30_nunique', 'B_38_count', 'B_38_nunique', 'D_114_count', ...]\n",
      "\t\t('int8', 'int')      : 397 | ['D_44_first', 'D_44_min', 'D_44_max', 'D_44_last', 'R_2_first', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 995 | ['P_2_first', 'P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', ...]\n",
      "\t\t('int', [])       : 373 | ['D_39_first', 'D_39_min', 'D_39_max', 'D_39_last', 'D_44_first', ...]\n",
      "\t\t('int', ['bool']) :  91 | ['R_2_first', 'R_2_min', 'R_2_max', 'R_2_last', 'S_6_first', ...]\n",
      "\t9.5s = Fit runtime\n",
      "\t1459 features in original data used to generate 1459 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 552.44 MB (11.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 10.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\learner.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\utils\\data\\X.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\utils\\data\\y.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tRandomForestGini_BAG_L1: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tRandomForestEntr_BAG_L1: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tExtraTreesGini_BAG_L1: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tExtraTreesEntr_BAG_L1: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_fit': {'num_gpus': 1}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}, 'ag_args_fit': {'num_gpus': 1}}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3589.75s of the 3589.73s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 12\n",
      "\tWarning: Model is expected to require 38.75% of available memory...\n",
      "\tNot enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3585.45s of the 3585.44s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 12\n",
      "\tWarning: Model is expected to require 38.73% of available memory...\n",
      "\tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3581.27s of the 3581.25s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 3.63 GB, but only 4.162 GB is available...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.258942\n",
      "[100]\tvalid_set's binary_logloss: 0.238168\n",
      "[150]\tvalid_set's binary_logloss: 0.234016\n",
      "[200]\tvalid_set's binary_logloss: 0.232483\n",
      "[250]\tvalid_set's binary_logloss: 0.231516\n",
      "[300]\tvalid_set's binary_logloss: 0.231058\n",
      "[350]\tvalid_set's binary_logloss: 0.231174\n",
      "[400]\tvalid_set's binary_logloss: 0.230998\n",
      "[450]\tvalid_set's binary_logloss: 0.231117\n",
      "[500]\tvalid_set's binary_logloss: 0.231191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.25661\n",
      "[100]\tvalid_set's binary_logloss: 0.234511\n",
      "[150]\tvalid_set's binary_logloss: 0.230028\n",
      "[200]\tvalid_set's binary_logloss: 0.228042\n",
      "[250]\tvalid_set's binary_logloss: 0.227648\n",
      "[300]\tvalid_set's binary_logloss: 0.227507\n",
      "[350]\tvalid_set's binary_logloss: 0.227548\n",
      "[400]\tvalid_set's binary_logloss: 0.227521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.254766\n",
      "[100]\tvalid_set's binary_logloss: 0.232237\n",
      "[150]\tvalid_set's binary_logloss: 0.227744\n",
      "[200]\tvalid_set's binary_logloss: 0.226133\n",
      "[250]\tvalid_set's binary_logloss: 0.225301\n",
      "[300]\tvalid_set's binary_logloss: 0.225016\n",
      "[350]\tvalid_set's binary_logloss: 0.225079\n",
      "[400]\tvalid_set's binary_logloss: 0.225116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.250297\n",
      "[100]\tvalid_set's binary_logloss: 0.22687\n",
      "[150]\tvalid_set's binary_logloss: 0.221932\n",
      "[200]\tvalid_set's binary_logloss: 0.219866\n",
      "[250]\tvalid_set's binary_logloss: 0.218925\n",
      "[300]\tvalid_set's binary_logloss: 0.218568\n",
      "[350]\tvalid_set's binary_logloss: 0.218326\n",
      "[400]\tvalid_set's binary_logloss: 0.218302\n",
      "[450]\tvalid_set's binary_logloss: 0.21844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.258708\n",
      "[100]\tvalid_set's binary_logloss: 0.235874\n",
      "[150]\tvalid_set's binary_logloss: 0.230529\n",
      "[200]\tvalid_set's binary_logloss: 0.228161\n",
      "[250]\tvalid_set's binary_logloss: 0.22712\n",
      "[300]\tvalid_set's binary_logloss: 0.226632\n",
      "[350]\tvalid_set's binary_logloss: 0.226442\n",
      "[400]\tvalid_set's binary_logloss: 0.226687\n",
      "[450]\tvalid_set's binary_logloss: 0.226719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 3.63 GB, but only 6.008 GB is available...\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.252232\n",
      "[100]\tvalid_set's binary_logloss: 0.228184\n",
      "[150]\tvalid_set's binary_logloss: 0.223305\n",
      "[200]\tvalid_set's binary_logloss: 0.221499\n",
      "[250]\tvalid_set's binary_logloss: 0.220416\n",
      "[300]\tvalid_set's binary_logloss: 0.219972\n",
      "[350]\tvalid_set's binary_logloss: 0.219431\n",
      "[400]\tvalid_set's binary_logloss: 0.219612\n",
      "[450]\tvalid_set's binary_logloss: 0.219301\n",
      "[500]\tvalid_set's binary_logloss: 0.219012\n",
      "[550]\tvalid_set's binary_logloss: 0.21904\n",
      "[600]\tvalid_set's binary_logloss: 0.218819\n",
      "[650]\tvalid_set's binary_logloss: 0.218866\n",
      "[700]\tvalid_set's binary_logloss: 0.218988\n",
      "[750]\tvalid_set's binary_logloss: 0.218864\n",
      "[800]\tvalid_set's binary_logloss: 0.218916\n",
      "[850]\tvalid_set's binary_logloss: 0.218937\n",
      "[900]\tvalid_set's binary_logloss: 0.218969\n",
      "[950]\tvalid_set's binary_logloss: 0.219208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.249116\n",
      "[100]\tvalid_set's binary_logloss: 0.225324\n",
      "[150]\tvalid_set's binary_logloss: 0.220357\n",
      "[200]\tvalid_set's binary_logloss: 0.217719\n",
      "[250]\tvalid_set's binary_logloss: 0.217209\n",
      "[300]\tvalid_set's binary_logloss: 0.216437\n",
      "[350]\tvalid_set's binary_logloss: 0.216163\n",
      "[400]\tvalid_set's binary_logloss: 0.215965\n",
      "[450]\tvalid_set's binary_logloss: 0.215713\n",
      "[500]\tvalid_set's binary_logloss: 0.215807\n",
      "[550]\tvalid_set's binary_logloss: 0.215857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.255764\n",
      "[100]\tvalid_set's binary_logloss: 0.233575\n",
      "[150]\tvalid_set's binary_logloss: 0.228684\n",
      "[200]\tvalid_set's binary_logloss: 0.226722\n",
      "[250]\tvalid_set's binary_logloss: 0.225691\n",
      "[300]\tvalid_set's binary_logloss: 0.225239\n",
      "[350]\tvalid_set's binary_logloss: 0.225389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-0.2234\t = Validation score   (-log_loss)\n",
      "\t450.61s\t = Training   runtime\n",
      "\t2.06s\t = Validation runtime\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3121.13s of the 3121.12s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.258286\n",
      "[100]\tvalid_set's binary_logloss: 0.238367\n",
      "[150]\tvalid_set's binary_logloss: 0.234386\n",
      "[200]\tvalid_set's binary_logloss: 0.233022\n",
      "[250]\tvalid_set's binary_logloss: 0.23269\n",
      "[300]\tvalid_set's binary_logloss: 0.232461\n",
      "[350]\tvalid_set's binary_logloss: 0.232248\n",
      "[400]\tvalid_set's binary_logloss: 0.232004\n",
      "[450]\tvalid_set's binary_logloss: 0.231958\n",
      "[500]\tvalid_set's binary_logloss: 0.232129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.255437\n",
      "[100]\tvalid_set's binary_logloss: 0.233873\n",
      "[150]\tvalid_set's binary_logloss: 0.229789\n",
      "[200]\tvalid_set's binary_logloss: 0.228628\n",
      "[250]\tvalid_set's binary_logloss: 0.22857\n",
      "[300]\tvalid_set's binary_logloss: 0.228509\n",
      "[350]\tvalid_set's binary_logloss: 0.228526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.252558\n",
      "[100]\tvalid_set's binary_logloss: 0.232196\n",
      "[150]\tvalid_set's binary_logloss: 0.228252\n",
      "[200]\tvalid_set's binary_logloss: 0.227237\n",
      "[250]\tvalid_set's binary_logloss: 0.227096\n",
      "[300]\tvalid_set's binary_logloss: 0.226971\n",
      "[350]\tvalid_set's binary_logloss: 0.227131\n",
      "[400]\tvalid_set's binary_logloss: 0.227321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.248902\n",
      "[100]\tvalid_set's binary_logloss: 0.227378\n",
      "[150]\tvalid_set's binary_logloss: 0.222679\n",
      "[200]\tvalid_set's binary_logloss: 0.221207\n",
      "[250]\tvalid_set's binary_logloss: 0.220773\n",
      "[300]\tvalid_set's binary_logloss: 0.22066\n",
      "[350]\tvalid_set's binary_logloss: 0.22089\n",
      "[400]\tvalid_set's binary_logloss: 0.220803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.256678\n",
      "[100]\tvalid_set's binary_logloss: 0.235787\n",
      "[150]\tvalid_set's binary_logloss: 0.23148\n",
      "[200]\tvalid_set's binary_logloss: 0.229909\n",
      "[250]\tvalid_set's binary_logloss: 0.229615\n",
      "[300]\tvalid_set's binary_logloss: 0.229124\n",
      "[350]\tvalid_set's binary_logloss: 0.228842\n",
      "[400]\tvalid_set's binary_logloss: 0.229315\n",
      "[450]\tvalid_set's binary_logloss: 0.229487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.249003\n",
      "[100]\tvalid_set's binary_logloss: 0.226886\n",
      "[150]\tvalid_set's binary_logloss: 0.222874\n",
      "[200]\tvalid_set's binary_logloss: 0.221506\n",
      "[250]\tvalid_set's binary_logloss: 0.221111\n",
      "[300]\tvalid_set's binary_logloss: 0.220908\n",
      "[350]\tvalid_set's binary_logloss: 0.220751\n",
      "[400]\tvalid_set's binary_logloss: 0.22071\n",
      "[450]\tvalid_set's binary_logloss: 0.220649\n",
      "[500]\tvalid_set's binary_logloss: 0.220379\n",
      "[550]\tvalid_set's binary_logloss: 0.220627\n",
      "[600]\tvalid_set's binary_logloss: 0.220699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.247146\n",
      "[100]\tvalid_set's binary_logloss: 0.224914\n",
      "[150]\tvalid_set's binary_logloss: 0.220659\n",
      "[200]\tvalid_set's binary_logloss: 0.21872\n",
      "[250]\tvalid_set's binary_logloss: 0.218342\n",
      "[300]\tvalid_set's binary_logloss: 0.217941\n",
      "[350]\tvalid_set's binary_logloss: 0.217777\n",
      "[400]\tvalid_set's binary_logloss: 0.217523\n",
      "[450]\tvalid_set's binary_logloss: 0.217326\n",
      "[500]\tvalid_set's binary_logloss: 0.217123\n",
      "[550]\tvalid_set's binary_logloss: 0.217145\n",
      "[600]\tvalid_set's binary_logloss: 0.217346\n",
      "[650]\tvalid_set's binary_logloss: 0.217397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.254357\n",
      "[100]\tvalid_set's binary_logloss: 0.233718\n",
      "[150]\tvalid_set's binary_logloss: 0.229504\n",
      "[200]\tvalid_set's binary_logloss: 0.228011\n",
      "[250]\tvalid_set's binary_logloss: 0.227722\n",
      "[300]\tvalid_set's binary_logloss: 0.227764\n",
      "[350]\tvalid_set's binary_logloss: 0.22792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-0.2251\t = Validation score   (-log_loss)\n",
      "\t458.15s\t = Training   runtime\n",
      "\t2.02s\t = Validation runtime\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 2655.61s of the 2655.59s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\RandomForestGini_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\RandomForestGini_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 12\n",
      "C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "\tWarning: Exception caused RandomForestGini_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput contains NaN, infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 220, in _fit\n",
      "    self._fit_single(X=X, y=y, model_base=model_base, use_child_oof=use_child_oof, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 326, in _fit_single\n",
      "    model_base.fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 189, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 327, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 2650.4s of the 2650.38s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\RandomForestEntr_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\RandomForestEntr_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 12\n",
      "C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "\tWarning: Exception caused RandomForestEntr_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput contains NaN, infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 220, in _fit\n",
      "    self._fit_single(X=X, y=y, model_base=model_base, use_child_oof=use_child_oof, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 326, in _fit_single\n",
      "    model_base.fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 189, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 327, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2645.28s of the 2645.26s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 6\n",
      "C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6, 'task_type': 'GPU'}\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6321746\ttest: 0.6323393\tbest: 0.6323393 (0)\ttotal: 30.1ms\tremaining: 150ms\n",
      "5:\tlearn: 0.4440060\ttest: 0.4451320\tbest: 0.4451320 (5)\ttotal: 115ms\tremaining: 0us\n",
      "bestTest = 0.4451320492\n",
      "bestIteration = 5\n",
      "0:\tlearn: 0.6321745\ttest: 0.6323393\tbest: 0.6323393 (0)\ttotal: 17.1ms\tremaining: 10.3s\n",
      "20:\tlearn: 0.2849275\ttest: 0.2901732\tbest: 0.2901732 (20)\ttotal: 342ms\tremaining: 9.46s\n",
      "40:\tlearn: 0.2461535\ttest: 0.2549805\tbest: 0.2549805 (40)\ttotal: 672ms\tremaining: 9.19s\n",
      "60:\tlearn: 0.2349642\ttest: 0.2459038\tbest: 0.2459038 (60)\ttotal: 998ms\tremaining: 8.85s\n",
      "80:\tlearn: 0.2291684\ttest: 0.2417183\tbest: 0.2417183 (80)\ttotal: 1.32s\tremaining: 8.51s\n",
      "100:\tlearn: 0.2253073\ttest: 0.2394751\tbest: 0.2394751 (100)\ttotal: 1.65s\tremaining: 8.19s\n",
      "120:\tlearn: 0.2222217\ttest: 0.2378315\tbest: 0.2378315 (120)\ttotal: 1.98s\tremaining: 7.88s\n",
      "140:\tlearn: 0.2195656\ttest: 0.2364637\tbest: 0.2364637 (140)\ttotal: 2.31s\tremaining: 7.54s\n",
      "160:\tlearn: 0.2172589\ttest: 0.2354731\tbest: 0.2354731 (160)\ttotal: 2.63s\tremaining: 7.22s\n",
      "180:\tlearn: 0.2152204\ttest: 0.2346984\tbest: 0.2346984 (180)\ttotal: 2.96s\tremaining: 6.87s\n",
      "200:\tlearn: 0.2132427\ttest: 0.2342185\tbest: 0.2342185 (200)\ttotal: 3.28s\tremaining: 6.54s\n",
      "220:\tlearn: 0.2116303\ttest: 0.2337568\tbest: 0.2337568 (220)\ttotal: 3.59s\tremaining: 6.19s\n",
      "240:\tlearn: 0.2098414\ttest: 0.2334069\tbest: 0.2334069 (240)\ttotal: 3.91s\tremaining: 5.86s\n",
      "260:\tlearn: 0.2082892\ttest: 0.2329294\tbest: 0.2329294 (260)\ttotal: 4.23s\tremaining: 5.53s\n",
      "280:\tlearn: 0.2068067\ttest: 0.2328338\tbest: 0.2328309 (279)\ttotal: 4.54s\tremaining: 5.19s\n",
      "300:\tlearn: 0.2053618\ttest: 0.2325164\tbest: 0.2325164 (300)\ttotal: 4.86s\tremaining: 4.86s\n",
      "320:\tlearn: 0.2039953\ttest: 0.2322851\tbest: 0.2322806 (317)\ttotal: 5.17s\tremaining: 4.52s\n",
      "340:\tlearn: 0.2027079\ttest: 0.2320360\tbest: 0.2320360 (340)\ttotal: 5.47s\tremaining: 4.19s\n",
      "360:\tlearn: 0.2012986\ttest: 0.2318878\tbest: 0.2318789 (358)\ttotal: 5.79s\tremaining: 3.87s\n",
      "380:\tlearn: 0.1999835\ttest: 0.2318144\tbest: 0.2318144 (380)\ttotal: 6.11s\tremaining: 3.54s\n",
      "400:\tlearn: 0.1987098\ttest: 0.2316409\tbest: 0.2316402 (399)\ttotal: 6.41s\tremaining: 3.21s\n",
      "420:\tlearn: 0.1974338\ttest: 0.2315693\tbest: 0.2315693 (420)\ttotal: 6.72s\tremaining: 2.89s\n",
      "440:\tlearn: 0.1962462\ttest: 0.2314894\tbest: 0.2314894 (440)\ttotal: 7.04s\tremaining: 2.57s\n",
      "460:\tlearn: 0.1949689\ttest: 0.2314140\tbest: 0.2313966 (457)\ttotal: 7.35s\tremaining: 2.25s\n",
      "480:\tlearn: 0.1938566\ttest: 0.2313971\tbest: 0.2313741 (470)\ttotal: 7.67s\tremaining: 1.93s\n",
      "500:\tlearn: 0.1926157\ttest: 0.2313132\tbest: 0.2313102 (491)\ttotal: 7.98s\tremaining: 1.61s\n",
      "520:\tlearn: 0.1915220\ttest: 0.2312077\tbest: 0.2312077 (520)\ttotal: 8.29s\tremaining: 1.29s\n",
      "540:\tlearn: 0.1903411\ttest: 0.2310926\tbest: 0.2310926 (540)\ttotal: 8.6s\tremaining: 970ms\n",
      "560:\tlearn: 0.1893135\ttest: 0.2310293\tbest: 0.2309877 (558)\ttotal: 8.92s\tremaining: 652ms\n",
      "580:\tlearn: 0.1882315\ttest: 0.2310195\tbest: 0.2309683 (573)\ttotal: 9.23s\tremaining: 334ms\n",
      "600:\tlearn: 0.1872085\ttest: 0.2309557\tbest: 0.2309339 (587)\ttotal: 9.53s\tremaining: 15.9ms\n",
      "601:\tlearn: 0.1871577\ttest: 0.2309402\tbest: 0.2309339 (587)\ttotal: 9.55s\tremaining: 0us\n",
      "bestTest = 0.2309339216\n",
      "bestIteration = 587\n",
      "Shrink model to first 588 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6, 'task_type': 'GPU'}\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6327786\ttest: 0.6329727\tbest: 0.6329727 (0)\ttotal: 19.3ms\tremaining: 96.7ms\n",
      "5:\tlearn: 0.4459564\ttest: 0.4475773\tbest: 0.4475773 (5)\ttotal: 113ms\tremaining: 0us\n",
      "bestTest = 0.4475773283\n",
      "bestIteration = 5\n",
      "0:\tlearn: 0.6327786\ttest: 0.6329728\tbest: 0.6329728 (0)\ttotal: 16.4ms\tremaining: 14.2s\n",
      "20:\tlearn: 0.2853972\ttest: 0.2894156\tbest: 0.2894156 (20)\ttotal: 354ms\tremaining: 14.2s\n",
      "40:\tlearn: 0.2470526\ttest: 0.2527499\tbest: 0.2527499 (40)\ttotal: 685ms\tremaining: 13.8s\n",
      "60:\tlearn: 0.2360056\ttest: 0.2432800\tbest: 0.2432800 (60)\ttotal: 1.01s\tremaining: 13.3s\n",
      "80:\tlearn: 0.2300663\ttest: 0.2385384\tbest: 0.2385384 (80)\ttotal: 1.34s\tremaining: 12.9s\n",
      "100:\tlearn: 0.2262744\ttest: 0.2358567\tbest: 0.2358567 (100)\ttotal: 1.66s\tremaining: 12.5s\n",
      "120:\tlearn: 0.2229476\ttest: 0.2339332\tbest: 0.2339332 (120)\ttotal: 1.99s\tremaining: 12.2s\n",
      "140:\tlearn: 0.2202468\ttest: 0.2326779\tbest: 0.2326779 (140)\ttotal: 2.32s\tremaining: 11.9s\n",
      "160:\tlearn: 0.2179744\ttest: 0.2314685\tbest: 0.2314685 (160)\ttotal: 2.64s\tremaining: 11.6s\n",
      "180:\tlearn: 0.2159819\ttest: 0.2307707\tbest: 0.2307707 (180)\ttotal: 2.96s\tremaining: 11.2s\n",
      "200:\tlearn: 0.2141001\ttest: 0.2299969\tbest: 0.2299969 (200)\ttotal: 3.28s\tremaining: 10.9s\n",
      "220:\tlearn: 0.2122208\ttest: 0.2293830\tbest: 0.2293830 (220)\ttotal: 3.62s\tremaining: 10.5s\n",
      "240:\tlearn: 0.2105622\ttest: 0.2289574\tbest: 0.2289574 (240)\ttotal: 3.94s\tremaining: 10.2s\n",
      "260:\tlearn: 0.2091027\ttest: 0.2286677\tbest: 0.2286677 (260)\ttotal: 4.27s\tremaining: 9.89s\n",
      "280:\tlearn: 0.2076272\ttest: 0.2284103\tbest: 0.2284103 (280)\ttotal: 4.59s\tremaining: 9.54s\n",
      "300:\tlearn: 0.2061447\ttest: 0.2282280\tbest: 0.2282280 (300)\ttotal: 4.92s\tremaining: 9.22s\n",
      "320:\tlearn: 0.2046389\ttest: 0.2280329\tbest: 0.2280189 (318)\ttotal: 5.25s\tremaining: 8.89s\n",
      "340:\tlearn: 0.2031175\ttest: 0.2278446\tbest: 0.2278438 (334)\ttotal: 5.57s\tremaining: 8.56s\n",
      "360:\tlearn: 0.2017100\ttest: 0.2276565\tbest: 0.2276565 (360)\ttotal: 5.89s\tremaining: 8.23s\n",
      "380:\tlearn: 0.2003988\ttest: 0.2274267\tbest: 0.2274137 (379)\ttotal: 6.21s\tremaining: 7.89s\n",
      "400:\tlearn: 0.1991745\ttest: 0.2273398\tbest: 0.2273218 (396)\ttotal: 6.53s\tremaining: 7.56s\n",
      "420:\tlearn: 0.1979633\ttest: 0.2272998\tbest: 0.2272548 (417)\ttotal: 6.85s\tremaining: 7.22s\n",
      "440:\tlearn: 0.1966535\ttest: 0.2271450\tbest: 0.2271450 (440)\ttotal: 7.17s\tremaining: 6.89s\n",
      "460:\tlearn: 0.1954205\ttest: 0.2269782\tbest: 0.2269782 (460)\ttotal: 7.49s\tremaining: 6.57s\n",
      "480:\tlearn: 0.1941792\ttest: 0.2268415\tbest: 0.2268415 (480)\ttotal: 7.81s\tremaining: 6.23s\n",
      "500:\tlearn: 0.1929232\ttest: 0.2267388\tbest: 0.2267363 (498)\ttotal: 8.14s\tremaining: 5.91s\n",
      "520:\tlearn: 0.1919286\ttest: 0.2267464\tbest: 0.2266953 (516)\ttotal: 8.45s\tremaining: 5.58s\n",
      "540:\tlearn: 0.1908404\ttest: 0.2267402\tbest: 0.2266953 (516)\ttotal: 8.77s\tremaining: 5.25s\n",
      "560:\tlearn: 0.1896664\ttest: 0.2265973\tbest: 0.2265902 (559)\ttotal: 9.09s\tremaining: 4.93s\n",
      "580:\tlearn: 0.1887248\ttest: 0.2265023\tbest: 0.2265023 (580)\ttotal: 9.4s\tremaining: 4.59s\n",
      "600:\tlearn: 0.1878048\ttest: 0.2264966\tbest: 0.2264032 (589)\ttotal: 9.71s\tremaining: 4.26s\n",
      "620:\tlearn: 0.1868064\ttest: 0.2264274\tbest: 0.2264032 (589)\ttotal: 10s\tremaining: 3.94s\n",
      "640:\tlearn: 0.1857271\ttest: 0.2263272\tbest: 0.2263117 (638)\ttotal: 10.3s\tremaining: 3.62s\n",
      "660:\tlearn: 0.1846293\ttest: 0.2262025\tbest: 0.2262025 (660)\ttotal: 10.7s\tremaining: 3.29s\n",
      "680:\tlearn: 0.1835255\ttest: 0.2261265\tbest: 0.2261265 (680)\ttotal: 11s\tremaining: 2.97s\n",
      "700:\tlearn: 0.1824299\ttest: 0.2261773\tbest: 0.2261265 (680)\ttotal: 11.3s\tremaining: 2.65s\n",
      "720:\tlearn: 0.1814013\ttest: 0.2261556\tbest: 0.2261265 (680)\ttotal: 11.6s\tremaining: 2.32s\n",
      "bestTest = 0.2261265452\n",
      "bestIteration = 680\n",
      "Shrink model to first 681 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6, 'task_type': 'GPU'}\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6329481\ttest: 0.6332424\tbest: 0.6332424 (0)\ttotal: 16.5ms\tremaining: 82.7ms\n",
      "5:\tlearn: 0.4450554\ttest: 0.4467812\tbest: 0.4467812 (5)\ttotal: 97.9ms\tremaining: 0us\n",
      "bestTest = 0.4467811741\n",
      "bestIteration = 5\n",
      "0:\tlearn: 0.6329481\ttest: 0.6332424\tbest: 0.6332424 (0)\ttotal: 18.6ms\tremaining: 18.3s\n",
      "20:\tlearn: 0.2845126\ttest: 0.2877182\tbest: 0.2877182 (20)\ttotal: 348ms\tremaining: 16s\n",
      "40:\tlearn: 0.2473679\ttest: 0.2522195\tbest: 0.2522195 (40)\ttotal: 688ms\tremaining: 15.9s\n",
      "60:\tlearn: 0.2358814\ttest: 0.2421456\tbest: 0.2421456 (60)\ttotal: 1.02s\tremaining: 15.5s\n",
      "80:\tlearn: 0.2300325\ttest: 0.2375192\tbest: 0.2375192 (80)\ttotal: 1.34s\tremaining: 15s\n",
      "100:\tlearn: 0.2261644\ttest: 0.2353102\tbest: 0.2353102 (100)\ttotal: 1.66s\tremaining: 14.6s\n",
      "120:\tlearn: 0.2229580\ttest: 0.2333150\tbest: 0.2333150 (120)\ttotal: 1.99s\tremaining: 14.2s\n",
      "140:\tlearn: 0.2203396\ttest: 0.2321129\tbest: 0.2321129 (140)\ttotal: 2.31s\tremaining: 13.9s\n",
      "160:\tlearn: 0.2181153\ttest: 0.2311073\tbest: 0.2311073 (160)\ttotal: 2.63s\tremaining: 13.5s\n",
      "180:\tlearn: 0.2161497\ttest: 0.2305607\tbest: 0.2305607 (180)\ttotal: 2.95s\tremaining: 13.1s\n",
      "200:\tlearn: 0.2142191\ttest: 0.2298293\tbest: 0.2298293 (200)\ttotal: 3.27s\tremaining: 12.8s\n",
      "220:\tlearn: 0.2123697\ttest: 0.2292542\tbest: 0.2292542 (220)\ttotal: 3.59s\tremaining: 12.5s\n",
      "240:\tlearn: 0.2105656\ttest: 0.2288329\tbest: 0.2288001 (239)\ttotal: 3.92s\tremaining: 12.1s\n",
      "260:\tlearn: 0.2089272\ttest: 0.2283661\tbest: 0.2283661 (260)\ttotal: 4.24s\tremaining: 11.8s\n",
      "280:\tlearn: 0.2073185\ttest: 0.2279214\tbest: 0.2279214 (280)\ttotal: 4.56s\tremaining: 11.4s\n",
      "300:\tlearn: 0.2058273\ttest: 0.2279197\tbest: 0.2278752 (283)\ttotal: 4.87s\tremaining: 11.1s\n",
      "320:\tlearn: 0.2043637\ttest: 0.2276332\tbest: 0.2276332 (320)\ttotal: 5.19s\tremaining: 10.8s\n",
      "340:\tlearn: 0.2029698\ttest: 0.2274268\tbest: 0.2274215 (339)\ttotal: 5.5s\tremaining: 10.4s\n",
      "360:\tlearn: 0.2015545\ttest: 0.2273801\tbest: 0.2273801 (360)\ttotal: 5.81s\tremaining: 10.1s\n",
      "380:\tlearn: 0.2001951\ttest: 0.2271693\tbest: 0.2271559 (378)\ttotal: 6.13s\tremaining: 9.76s\n",
      "400:\tlearn: 0.1989286\ttest: 0.2271194\tbest: 0.2270979 (398)\ttotal: 6.45s\tremaining: 9.42s\n",
      "420:\tlearn: 0.1976893\ttest: 0.2269480\tbest: 0.2269480 (420)\ttotal: 6.76s\tremaining: 9.09s\n",
      "440:\tlearn: 0.1963413\ttest: 0.2268840\tbest: 0.2268557 (437)\ttotal: 7.08s\tremaining: 8.76s\n",
      "460:\tlearn: 0.1952401\ttest: 0.2268852\tbest: 0.2268557 (437)\ttotal: 7.38s\tremaining: 8.42s\n",
      "480:\tlearn: 0.1939903\ttest: 0.2268712\tbest: 0.2268139 (476)\ttotal: 7.69s\tremaining: 8.09s\n",
      "500:\tlearn: 0.1928866\ttest: 0.2268698\tbest: 0.2268139 (476)\ttotal: 8.01s\tremaining: 7.77s\n",
      "520:\tlearn: 0.1917194\ttest: 0.2268296\tbest: 0.2268139 (476)\ttotal: 8.32s\tremaining: 7.44s\n",
      "bestTest = 0.2268138544\n",
      "bestIteration = 476\n",
      "Shrink model to first 477 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6, 'task_type': 'GPU'}\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6322120\ttest: 0.6320243\tbest: 0.6320243 (0)\ttotal: 17.4ms\tremaining: 87.1ms\n",
      "5:\tlearn: 0.4426751\ttest: 0.4412938\tbest: 0.4412938 (5)\ttotal: 103ms\tremaining: 0us\n",
      "bestTest = 0.4412938087\n",
      "bestIteration = 5\n",
      "0:\tlearn: 0.6322119\ttest: 0.6320243\tbest: 0.6320243 (0)\ttotal: 17.1ms\tremaining: 20s\n",
      "20:\tlearn: 0.2858390\ttest: 0.2833009\tbest: 0.2833009 (20)\ttotal: 344ms\tremaining: 18.9s\n",
      "40:\tlearn: 0.2477056\ttest: 0.2455377\tbest: 0.2455377 (40)\ttotal: 679ms\tremaining: 18.8s\n",
      "60:\tlearn: 0.2370957\ttest: 0.2356347\tbest: 0.2356347 (60)\ttotal: 1s\tremaining: 18.3s\n",
      "80:\tlearn: 0.2310746\ttest: 0.2304938\tbest: 0.2304938 (80)\ttotal: 1.33s\tremaining: 17.9s\n",
      "100:\tlearn: 0.2271847\ttest: 0.2276514\tbest: 0.2276514 (100)\ttotal: 1.66s\tremaining: 17.6s\n",
      "120:\tlearn: 0.2241200\ttest: 0.2259435\tbest: 0.2259435 (120)\ttotal: 1.98s\tremaining: 17.3s\n",
      "140:\tlearn: 0.2215147\ttest: 0.2247102\tbest: 0.2247102 (140)\ttotal: 2.31s\tremaining: 16.9s\n",
      "160:\tlearn: 0.2191401\ttest: 0.2237610\tbest: 0.2237610 (160)\ttotal: 2.63s\tremaining: 16.6s\n",
      "180:\tlearn: 0.2170542\ttest: 0.2229392\tbest: 0.2229392 (180)\ttotal: 2.96s\tremaining: 16.2s\n",
      "200:\tlearn: 0.2152177\ttest: 0.2223544\tbest: 0.2223544 (200)\ttotal: 3.28s\tremaining: 15.9s\n",
      "220:\tlearn: 0.2134500\ttest: 0.2218807\tbest: 0.2218807 (220)\ttotal: 3.6s\tremaining: 15.5s\n",
      "240:\tlearn: 0.2116983\ttest: 0.2215897\tbest: 0.2215765 (238)\ttotal: 3.95s\tremaining: 15.3s\n",
      "260:\tlearn: 0.2100915\ttest: 0.2213989\tbest: 0.2213989 (260)\ttotal: 4.27s\tremaining: 14.9s\n",
      "280:\tlearn: 0.2084952\ttest: 0.2211387\tbest: 0.2211387 (280)\ttotal: 4.59s\tremaining: 14.6s\n",
      "300:\tlearn: 0.2069663\ttest: 0.2208268\tbest: 0.2207951 (296)\ttotal: 4.91s\tremaining: 14.3s\n",
      "320:\tlearn: 0.2054057\ttest: 0.2205281\tbest: 0.2205274 (319)\ttotal: 5.23s\tremaining: 13.9s\n",
      "340:\tlearn: 0.2040404\ttest: 0.2203904\tbest: 0.2203904 (340)\ttotal: 5.55s\tremaining: 13.6s\n",
      "360:\tlearn: 0.2027007\ttest: 0.2202900\tbest: 0.2202900 (360)\ttotal: 5.87s\tremaining: 13.2s\n",
      "380:\tlearn: 0.2013537\ttest: 0.2200476\tbest: 0.2200476 (380)\ttotal: 6.19s\tremaining: 12.9s\n",
      "400:\tlearn: 0.2000681\ttest: 0.2198261\tbest: 0.2198261 (400)\ttotal: 6.5s\tremaining: 12.5s\n",
      "420:\tlearn: 0.1988974\ttest: 0.2196794\tbest: 0.2196794 (420)\ttotal: 6.81s\tremaining: 12.2s\n",
      "440:\tlearn: 0.1977614\ttest: 0.2195715\tbest: 0.2195715 (440)\ttotal: 7.12s\tremaining: 11.8s\n",
      "460:\tlearn: 0.1966625\ttest: 0.2195126\tbest: 0.2195074 (459)\ttotal: 7.43s\tremaining: 11.5s\n",
      "480:\tlearn: 0.1954562\ttest: 0.2194280\tbest: 0.2194258 (476)\ttotal: 7.75s\tremaining: 11.2s\n",
      "500:\tlearn: 0.1942779\ttest: 0.2193020\tbest: 0.2192725 (498)\ttotal: 8.07s\tremaining: 10.8s\n",
      "520:\tlearn: 0.1932071\ttest: 0.2191698\tbest: 0.2191563 (518)\ttotal: 8.39s\tremaining: 10.5s\n",
      "540:\tlearn: 0.1921121\ttest: 0.2191222\tbest: 0.2190770 (535)\ttotal: 8.69s\tremaining: 10.2s\n",
      "560:\tlearn: 0.1909747\ttest: 0.2191274\tbest: 0.2190563 (551)\ttotal: 9.01s\tremaining: 9.85s\n",
      "580:\tlearn: 0.1898537\ttest: 0.2190720\tbest: 0.2190466 (577)\ttotal: 9.32s\tremaining: 9.51s\n",
      "600:\tlearn: 0.1887906\ttest: 0.2189619\tbest: 0.2189585 (597)\ttotal: 9.64s\tremaining: 9.19s\n",
      "620:\tlearn: 0.1877068\ttest: 0.2189471\tbest: 0.2189418 (617)\ttotal: 9.96s\tremaining: 8.87s\n",
      "640:\tlearn: 0.1867145\ttest: 0.2188858\tbest: 0.2188548 (636)\ttotal: 10.3s\tremaining: 8.54s\n",
      "660:\tlearn: 0.1856383\ttest: 0.2190029\tbest: 0.2188548 (636)\ttotal: 10.6s\tremaining: 8.21s\n",
      "680:\tlearn: 0.1844810\ttest: 0.2189486\tbest: 0.2188548 (636)\ttotal: 10.9s\tremaining: 7.89s\n",
      "bestTest = 0.2188548019\n",
      "bestIteration = 636\n",
      "Shrink model to first 637 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6, 'task_type': 'GPU'}\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6322037\ttest: 0.6327873\tbest: 0.6327873 (0)\ttotal: 19.8ms\tremaining: 99.1ms\n",
      "5:\tlearn: 0.4442640\ttest: 0.4464917\tbest: 0.4464917 (5)\ttotal: 118ms\tremaining: 0us\n",
      "bestTest = 0.4464916867\n",
      "bestIteration = 5\n",
      "0:\tlearn: 0.6322037\ttest: 0.6327873\tbest: 0.6327873 (0)\ttotal: 17.6ms\tremaining: 25.2s\n",
      "20:\tlearn: 0.2842196\ttest: 0.2897653\tbest: 0.2897653 (20)\ttotal: 346ms\tremaining: 23.2s\n",
      "40:\tlearn: 0.2468974\ttest: 0.2543866\tbest: 0.2543866 (40)\ttotal: 683ms\tremaining: 23.1s\n",
      "60:\tlearn: 0.2354263\ttest: 0.2445418\tbest: 0.2445418 (60)\ttotal: 1.01s\tremaining: 22.7s\n",
      "80:\tlearn: 0.2295682\ttest: 0.2402996\tbest: 0.2402996 (80)\ttotal: 1.34s\tremaining: 22.3s\n",
      "100:\tlearn: 0.2258859\ttest: 0.2377923\tbest: 0.2377923 (100)\ttotal: 1.67s\tremaining: 21.9s\n",
      "120:\tlearn: 0.2226453\ttest: 0.2356550\tbest: 0.2356550 (120)\ttotal: 2s\tremaining: 21.6s\n",
      "140:\tlearn: 0.2199212\ttest: 0.2341103\tbest: 0.2341103 (140)\ttotal: 2.32s\tremaining: 21.2s\n",
      "160:\tlearn: 0.2178004\ttest: 0.2333032\tbest: 0.2333032 (160)\ttotal: 2.65s\tremaining: 20.9s\n",
      "180:\tlearn: 0.2158874\ttest: 0.2325792\tbest: 0.2325792 (180)\ttotal: 2.98s\tremaining: 20.5s\n",
      "200:\tlearn: 0.2140105\ttest: 0.2318685\tbest: 0.2318685 (200)\ttotal: 3.29s\tremaining: 20.1s\n",
      "220:\tlearn: 0.2123467\ttest: 0.2313748\tbest: 0.2313748 (220)\ttotal: 3.61s\tremaining: 19.7s\n",
      "240:\tlearn: 0.2107319\ttest: 0.2309810\tbest: 0.2309810 (240)\ttotal: 3.93s\tremaining: 19.3s\n",
      "260:\tlearn: 0.2090786\ttest: 0.2306248\tbest: 0.2305964 (259)\ttotal: 4.24s\tremaining: 19s\n",
      "280:\tlearn: 0.2074050\ttest: 0.2301759\tbest: 0.2301759 (280)\ttotal: 4.56s\tremaining: 18.6s\n",
      "300:\tlearn: 0.2060428\ttest: 0.2299644\tbest: 0.2299644 (300)\ttotal: 4.87s\tremaining: 18.2s\n",
      "320:\tlearn: 0.2045648\ttest: 0.2296923\tbest: 0.2296731 (319)\ttotal: 5.18s\tremaining: 17.9s\n",
      "340:\tlearn: 0.2031440\ttest: 0.2293222\tbest: 0.2293200 (339)\ttotal: 5.5s\tremaining: 17.5s\n",
      "360:\tlearn: 0.2018419\ttest: 0.2289047\tbest: 0.2289047 (360)\ttotal: 5.82s\tremaining: 17.2s\n",
      "380:\tlearn: 0.2005566\ttest: 0.2287277\tbest: 0.2287277 (380)\ttotal: 6.13s\tremaining: 16.8s\n",
      "400:\tlearn: 0.1992598\ttest: 0.2285666\tbest: 0.2285449 (394)\ttotal: 6.44s\tremaining: 16.5s\n",
      "420:\tlearn: 0.1980266\ttest: 0.2284216\tbest: 0.2284216 (420)\ttotal: 6.75s\tremaining: 16.2s\n",
      "440:\tlearn: 0.1968167\ttest: 0.2283235\tbest: 0.2282558 (435)\ttotal: 7.06s\tremaining: 15.8s\n",
      "460:\tlearn: 0.1955783\ttest: 0.2281461\tbest: 0.2281461 (460)\ttotal: 7.38s\tremaining: 15.5s\n",
      "480:\tlearn: 0.1945582\ttest: 0.2279917\tbest: 0.2279784 (477)\ttotal: 7.68s\tremaining: 15.1s\n",
      "500:\tlearn: 0.1933179\ttest: 0.2277415\tbest: 0.2277415 (500)\ttotal: 8s\tremaining: 14.8s\n",
      "520:\tlearn: 0.1922620\ttest: 0.2274749\tbest: 0.2274573 (518)\ttotal: 8.3s\tremaining: 14.4s\n",
      "540:\tlearn: 0.1912160\ttest: 0.2273109\tbest: 0.2273109 (540)\ttotal: 8.61s\tremaining: 14.1s\n",
      "560:\tlearn: 0.1900982\ttest: 0.2271925\tbest: 0.2271754 (559)\ttotal: 8.93s\tremaining: 13.8s\n",
      "580:\tlearn: 0.1890309\ttest: 0.2270028\tbest: 0.2270028 (580)\ttotal: 9.23s\tremaining: 13.5s\n",
      "600:\tlearn: 0.1877959\ttest: 0.2267588\tbest: 0.2267588 (600)\ttotal: 9.55s\tremaining: 13.1s\n",
      "620:\tlearn: 0.1867461\ttest: 0.2266649\tbest: 0.2266442 (611)\ttotal: 9.86s\tremaining: 12.8s\n",
      "640:\tlearn: 0.1856263\ttest: 0.2267889\tbest: 0.2266442 (611)\ttotal: 10.2s\tremaining: 12.5s\n",
      "660:\tlearn: 0.1846585\ttest: 0.2267407\tbest: 0.2266442 (611)\ttotal: 10.5s\tremaining: 12.2s\n",
      "bestTest = 0.2266442136\n",
      "bestIteration = 611\n",
      "Shrink model to first 612 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6, 'task_type': 'GPU'}\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6331816\ttest: 0.6332578\tbest: 0.6332578 (0)\ttotal: 16.7ms\tremaining: 83.6ms\n",
      "5:\tlearn: 0.4456518\ttest: 0.4448772\tbest: 0.4448772 (5)\ttotal: 98.5ms\tremaining: 0us\n",
      "bestTest = 0.4448772049\n",
      "bestIteration = 5\n",
      "0:\tlearn: 0.6331816\ttest: 0.6332578\tbest: 0.6332578 (0)\ttotal: 16.6ms\tremaining: 32.5s\n",
      "20:\tlearn: 0.2865067\ttest: 0.2847079\tbest: 0.2847079 (20)\ttotal: 481ms\tremaining: 44.2s\n",
      "40:\tlearn: 0.2485790\ttest: 0.2467482\tbest: 0.2467482 (40)\ttotal: 857ms\tremaining: 40s\n",
      "60:\tlearn: 0.2369809\ttest: 0.2362120\tbest: 0.2362120 (60)\ttotal: 1.2s\tremaining: 37.1s\n",
      "80:\tlearn: 0.2310884\ttest: 0.2314922\tbest: 0.2314922 (80)\ttotal: 1.54s\tremaining: 35.5s\n",
      "100:\tlearn: 0.2271359\ttest: 0.2289186\tbest: 0.2289186 (100)\ttotal: 1.86s\tremaining: 34.2s\n",
      "120:\tlearn: 0.2239948\ttest: 0.2271387\tbest: 0.2271387 (120)\ttotal: 2.19s\tremaining: 33.2s\n",
      "140:\tlearn: 0.2213819\ttest: 0.2257854\tbest: 0.2257854 (140)\ttotal: 2.51s\tremaining: 32.3s\n",
      "160:\tlearn: 0.2191385\ttest: 0.2248493\tbest: 0.2248493 (160)\ttotal: 2.84s\tremaining: 31.7s\n",
      "180:\tlearn: 0.2170015\ttest: 0.2239903\tbest: 0.2239903 (180)\ttotal: 3.17s\tremaining: 31s\n",
      "200:\tlearn: 0.2150928\ttest: 0.2232104\tbest: 0.2232104 (200)\ttotal: 3.49s\tremaining: 30.4s\n",
      "220:\tlearn: 0.2133553\ttest: 0.2227119\tbest: 0.2227119 (220)\ttotal: 3.81s\tremaining: 29.9s\n",
      "240:\tlearn: 0.2117967\ttest: 0.2222314\tbest: 0.2222314 (240)\ttotal: 4.16s\tremaining: 29.5s\n",
      "260:\tlearn: 0.2101288\ttest: 0.2217867\tbest: 0.2217867 (260)\ttotal: 4.56s\tremaining: 29.6s\n",
      "280:\tlearn: 0.2086035\ttest: 0.2215664\tbest: 0.2215405 (279)\ttotal: 4.89s\tremaining: 29.1s\n",
      "300:\tlearn: 0.2071758\ttest: 0.2212847\tbest: 0.2212847 (300)\ttotal: 5.22s\tremaining: 28.6s\n",
      "320:\tlearn: 0.2057737\ttest: 0.2210093\tbest: 0.2210093 (320)\ttotal: 5.61s\tremaining: 28.5s\n",
      "340:\tlearn: 0.2043242\ttest: 0.2206717\tbest: 0.2206717 (340)\ttotal: 6.06s\tremaining: 28.7s\n",
      "360:\tlearn: 0.2029154\ttest: 0.2204243\tbest: 0.2204003 (359)\ttotal: 6.46s\tremaining: 28.5s\n",
      "380:\tlearn: 0.2015187\ttest: 0.2202459\tbest: 0.2202267 (379)\ttotal: 6.78s\tremaining: 28s\n",
      "400:\tlearn: 0.2001909\ttest: 0.2200964\tbest: 0.2200579 (399)\ttotal: 7.1s\tremaining: 27.5s\n",
      "420:\tlearn: 0.1988795\ttest: 0.2199087\tbest: 0.2199087 (420)\ttotal: 7.42s\tremaining: 27s\n",
      "440:\tlearn: 0.1977969\ttest: 0.2197733\tbest: 0.2197636 (438)\ttotal: 7.73s\tremaining: 26.5s\n",
      "460:\tlearn: 0.1965617\ttest: 0.2195763\tbest: 0.2195763 (460)\ttotal: 8.04s\tremaining: 26s\n",
      "480:\tlearn: 0.1952761\ttest: 0.2193914\tbest: 0.2193914 (480)\ttotal: 8.36s\tremaining: 25.6s\n",
      "500:\tlearn: 0.1940758\ttest: 0.2193534\tbest: 0.2193304 (491)\ttotal: 8.67s\tremaining: 25.1s\n",
      "520:\tlearn: 0.1930049\ttest: 0.2192375\tbest: 0.2192375 (520)\ttotal: 8.98s\tremaining: 24.7s\n",
      "540:\tlearn: 0.1918556\ttest: 0.2190772\tbest: 0.2190549 (535)\ttotal: 9.29s\tremaining: 24.2s\n",
      "560:\tlearn: 0.1907711\ttest: 0.2189302\tbest: 0.2189302 (560)\ttotal: 9.6s\tremaining: 23.8s\n",
      "580:\tlearn: 0.1896992\ttest: 0.2186631\tbest: 0.2186631 (580)\ttotal: 9.9s\tremaining: 23.4s\n",
      "600:\tlearn: 0.1886055\ttest: 0.2186201\tbest: 0.2186201 (600)\ttotal: 10.2s\tremaining: 23s\n",
      "620:\tlearn: 0.1875834\ttest: 0.2186291\tbest: 0.2186095 (604)\ttotal: 10.5s\tremaining: 22.6s\n",
      "640:\tlearn: 0.1864898\ttest: 0.2184549\tbest: 0.2184496 (636)\ttotal: 10.8s\tremaining: 22.2s\n",
      "660:\tlearn: 0.1854236\ttest: 0.2184606\tbest: 0.2184117 (645)\ttotal: 11.2s\tremaining: 21.8s\n",
      "680:\tlearn: 0.1844759\ttest: 0.2182844\tbest: 0.2182844 (680)\ttotal: 11.5s\tremaining: 21.4s\n",
      "700:\tlearn: 0.1834048\ttest: 0.2180730\tbest: 0.2180658 (698)\ttotal: 11.8s\tremaining: 21s\n",
      "720:\tlearn: 0.1822962\ttest: 0.2180302\tbest: 0.2180074 (708)\ttotal: 12.2s\tremaining: 20.8s\n",
      "740:\tlearn: 0.1812866\ttest: 0.2179504\tbest: 0.2179504 (740)\ttotal: 12.5s\tremaining: 20.4s\n",
      "760:\tlearn: 0.1801785\ttest: 0.2178998\tbest: 0.2178372 (755)\ttotal: 12.8s\tremaining: 20.1s\n",
      "780:\tlearn: 0.1791378\ttest: 0.2178165\tbest: 0.2178165 (780)\ttotal: 13.1s\tremaining: 19.7s\n",
      "800:\tlearn: 0.1781371\ttest: 0.2178111\tbest: 0.2178023 (795)\ttotal: 13.5s\tremaining: 19.3s\n",
      "820:\tlearn: 0.1770616\ttest: 0.2177170\tbest: 0.2176893 (818)\ttotal: 13.8s\tremaining: 19.1s\n",
      "840:\tlearn: 0.1760843\ttest: 0.2177208\tbest: 0.2176893 (818)\ttotal: 14.2s\tremaining: 18.8s\n",
      "860:\tlearn: 0.1751103\ttest: 0.2175829\tbest: 0.2175829 (860)\ttotal: 14.6s\tremaining: 18.5s\n",
      "880:\tlearn: 0.1741705\ttest: 0.2174501\tbest: 0.2174501 (880)\ttotal: 15s\tremaining: 18.2s\n",
      "900:\tlearn: 0.1732266\ttest: 0.2175122\tbest: 0.2174379 (887)\ttotal: 15.3s\tremaining: 17.9s\n",
      "920:\tlearn: 0.1723980\ttest: 0.2174889\tbest: 0.2174379 (887)\ttotal: 15.6s\tremaining: 17.5s\n",
      "bestTest = 0.2174379436\n",
      "bestIteration = 887\n",
      "Shrink model to first 888 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6, 'task_type': 'GPU'}\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6313060\ttest: 0.6312067\tbest: 0.6312067 (0)\ttotal: 26ms\tremaining: 130ms\n",
      "5:\tlearn: 0.4433239\ttest: 0.4428508\tbest: 0.4428508 (5)\ttotal: 151ms\tremaining: 0us\n",
      "bestTest = 0.442850793\n",
      "bestIteration = 5\n",
      "0:\tlearn: 0.6313059\ttest: 0.6312067\tbest: 0.6312067 (0)\ttotal: 17.4ms\tremaining: 46.9s\n",
      "20:\tlearn: 0.2858379\ttest: 0.2833130\tbest: 0.2833130 (20)\ttotal: 354ms\tremaining: 45.2s\n",
      "40:\tlearn: 0.2483749\ttest: 0.2459000\tbest: 0.2459000 (40)\ttotal: 707ms\tremaining: 45.9s\n",
      "60:\tlearn: 0.2369842\ttest: 0.2353241\tbest: 0.2353241 (60)\ttotal: 1.04s\tremaining: 45s\n",
      "80:\tlearn: 0.2309686\ttest: 0.2302580\tbest: 0.2302580 (80)\ttotal: 1.42s\tremaining: 46s\n",
      "100:\tlearn: 0.2271876\ttest: 0.2278036\tbest: 0.2278036 (100)\ttotal: 1.75s\tremaining: 45.2s\n",
      "120:\tlearn: 0.2242357\ttest: 0.2260779\tbest: 0.2260779 (120)\ttotal: 2.09s\tremaining: 44.6s\n",
      "140:\tlearn: 0.2215108\ttest: 0.2245809\tbest: 0.2245809 (140)\ttotal: 2.42s\tremaining: 44s\n",
      "160:\tlearn: 0.2191782\ttest: 0.2235392\tbest: 0.2235392 (160)\ttotal: 2.74s\tremaining: 43.3s\n",
      "180:\tlearn: 0.2170942\ttest: 0.2225869\tbest: 0.2225841 (179)\ttotal: 3.07s\tremaining: 42.7s\n",
      "200:\tlearn: 0.2151616\ttest: 0.2218172\tbest: 0.2217864 (199)\ttotal: 3.38s\tremaining: 42.1s\n",
      "220:\tlearn: 0.2134367\ttest: 0.2215433\tbest: 0.2215376 (218)\ttotal: 3.73s\tremaining: 41.9s\n",
      "240:\tlearn: 0.2116464\ttest: 0.2210150\tbest: 0.2210150 (240)\ttotal: 4.06s\tremaining: 41.5s\n",
      "260:\tlearn: 0.2100830\ttest: 0.2207408\tbest: 0.2207408 (260)\ttotal: 4.37s\tremaining: 40.9s\n",
      "280:\tlearn: 0.2085311\ttest: 0.2205235\tbest: 0.2205235 (280)\ttotal: 4.69s\tremaining: 40.4s\n",
      "300:\tlearn: 0.2070106\ttest: 0.2201466\tbest: 0.2201466 (300)\ttotal: 5s\tremaining: 40s\n",
      "320:\tlearn: 0.2055656\ttest: 0.2200266\tbest: 0.2200112 (319)\ttotal: 5.32s\tremaining: 39.5s\n",
      "340:\tlearn: 0.2041055\ttest: 0.2197501\tbest: 0.2197501 (340)\ttotal: 5.69s\tremaining: 39.4s\n",
      "360:\tlearn: 0.2027332\ttest: 0.2195589\tbest: 0.2195589 (360)\ttotal: 6.03s\tremaining: 39.2s\n",
      "380:\tlearn: 0.2013668\ttest: 0.2194221\tbest: 0.2194218 (378)\ttotal: 6.39s\tremaining: 38.9s\n",
      "400:\tlearn: 0.2000859\ttest: 0.2192221\tbest: 0.2192221 (400)\ttotal: 6.74s\tremaining: 38.7s\n",
      "420:\tlearn: 0.1988285\ttest: 0.2188185\tbest: 0.2188185 (420)\ttotal: 7.11s\tremaining: 38.5s\n",
      "440:\tlearn: 0.1974950\ttest: 0.2186030\tbest: 0.2185892 (439)\ttotal: 7.43s\tremaining: 38.1s\n",
      "460:\tlearn: 0.1963497\ttest: 0.2185493\tbest: 0.2185086 (450)\ttotal: 7.74s\tremaining: 37.7s\n",
      "480:\tlearn: 0.1951293\ttest: 0.2184335\tbest: 0.2184182 (477)\ttotal: 8.08s\tremaining: 37.4s\n",
      "500:\tlearn: 0.1939287\ttest: 0.2184015\tbest: 0.2182947 (490)\ttotal: 8.4s\tremaining: 36.9s\n",
      "520:\tlearn: 0.1927582\ttest: 0.2181643\tbest: 0.2181601 (519)\ttotal: 8.72s\tremaining: 36.5s\n",
      "540:\tlearn: 0.1915229\ttest: 0.2180695\tbest: 0.2180595 (535)\ttotal: 9.06s\tremaining: 36.2s\n",
      "560:\tlearn: 0.1904889\ttest: 0.2179405\tbest: 0.2179405 (560)\ttotal: 9.4s\tremaining: 35.9s\n",
      "580:\tlearn: 0.1893938\ttest: 0.2178376\tbest: 0.2178314 (579)\ttotal: 9.73s\tremaining: 35.5s\n",
      "600:\tlearn: 0.1882749\ttest: 0.2177472\tbest: 0.2177121 (598)\ttotal: 10.1s\tremaining: 35.4s\n",
      "620:\tlearn: 0.1871259\ttest: 0.2177199\tbest: 0.2177121 (598)\ttotal: 10.5s\tremaining: 35.3s\n",
      "640:\tlearn: 0.1861495\ttest: 0.2176707\tbest: 0.2176412 (634)\ttotal: 11s\tremaining: 35.3s\n",
      "660:\tlearn: 0.1851206\ttest: 0.2175351\tbest: 0.2175243 (659)\ttotal: 11.4s\tremaining: 35.2s\n",
      "680:\tlearn: 0.1841178\ttest: 0.2174934\tbest: 0.2174771 (679)\ttotal: 11.8s\tremaining: 35.1s\n",
      "700:\tlearn: 0.1830743\ttest: 0.2174851\tbest: 0.2174326 (683)\ttotal: 12.3s\tremaining: 35.1s\n",
      "720:\tlearn: 0.1821377\ttest: 0.2174410\tbest: 0.2174326 (683)\ttotal: 12.7s\tremaining: 34.8s\n",
      "740:\tlearn: 0.1810668\ttest: 0.2172426\tbest: 0.2172355 (737)\ttotal: 13s\tremaining: 34.4s\n",
      "760:\tlearn: 0.1800579\ttest: 0.2172006\tbest: 0.2171569 (753)\ttotal: 13.3s\tremaining: 34s\n",
      "780:\tlearn: 0.1789551\ttest: 0.2171268\tbest: 0.2171115 (770)\ttotal: 13.6s\tremaining: 33.6s\n",
      "800:\tlearn: 0.1779810\ttest: 0.2171415\tbest: 0.2171049 (793)\ttotal: 14s\tremaining: 33.2s\n",
      "820:\tlearn: 0.1769712\ttest: 0.2171468\tbest: 0.2170881 (808)\ttotal: 14.3s\tremaining: 32.8s\n",
      "840:\tlearn: 0.1759864\ttest: 0.2170763\tbest: 0.2170366 (831)\ttotal: 14.6s\tremaining: 32.3s\n",
      "860:\tlearn: 0.1751259\ttest: 0.2169915\tbest: 0.2169783 (850)\ttotal: 14.9s\tremaining: 31.9s\n",
      "880:\tlearn: 0.1741732\ttest: 0.2169355\tbest: 0.2169295 (877)\ttotal: 15.3s\tremaining: 31.6s\n",
      "900:\tlearn: 0.1731730\ttest: 0.2170343\tbest: 0.2169295 (877)\ttotal: 15.6s\tremaining: 31.2s\n",
      "920:\tlearn: 0.1722012\ttest: 0.2169308\tbest: 0.2169180 (918)\ttotal: 15.9s\tremaining: 30.8s\n",
      "940:\tlearn: 0.1713709\ttest: 0.2170198\tbest: 0.2169180 (918)\ttotal: 16.2s\tremaining: 30.3s\n",
      "960:\tlearn: 0.1703276\ttest: 0.2170494\tbest: 0.2169180 (918)\ttotal: 16.6s\tremaining: 30s\n",
      "bestTest = 0.2169179558\n",
      "bestIteration = 918\n",
      "Shrink model to first 919 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6, 'task_type': 'GPU'}\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6353956\ttest: 0.6355114\tbest: 0.6355114 (0)\ttotal: 17.1ms\tremaining: 85.3ms\n",
      "5:\tlearn: 0.4447061\ttest: 0.4454042\tbest: 0.4454042 (5)\ttotal: 99ms\tremaining: 0us\n",
      "bestTest = 0.4454041804\n",
      "bestIteration = 5\n",
      "0:\tlearn: 0.6353955\ttest: 0.6355114\tbest: 0.6355114 (0)\ttotal: 17.2ms\tremaining: 1m 32s\n",
      "20:\tlearn: 0.2843731\ttest: 0.2870348\tbest: 0.2870348 (20)\ttotal: 349ms\tremaining: 1m 29s\n",
      "40:\tlearn: 0.2469997\ttest: 0.2516278\tbest: 0.2516278 (40)\ttotal: 687ms\tremaining: 1m 29s\n",
      "60:\tlearn: 0.2357404\ttest: 0.2421019\tbest: 0.2421019 (60)\ttotal: 1.03s\tremaining: 1m 29s\n",
      "80:\tlearn: 0.2300133\ttest: 0.2377712\tbest: 0.2377712 (80)\ttotal: 1.36s\tremaining: 1m 29s\n",
      "100:\tlearn: 0.2259631\ttest: 0.2355029\tbest: 0.2355029 (100)\ttotal: 1.71s\tremaining: 1m 29s\n",
      "120:\tlearn: 0.2230056\ttest: 0.2339876\tbest: 0.2339876 (120)\ttotal: 2.03s\tremaining: 1m 28s\n",
      "140:\tlearn: 0.2202834\ttest: 0.2327884\tbest: 0.2327884 (140)\ttotal: 2.36s\tremaining: 1m 27s\n",
      "160:\tlearn: 0.2180271\ttest: 0.2319841\tbest: 0.2319841 (160)\ttotal: 2.69s\tremaining: 1m 27s\n",
      "180:\tlearn: 0.2157020\ttest: 0.2310811\tbest: 0.2310811 (180)\ttotal: 3.04s\tremaining: 1m 27s\n",
      "200:\tlearn: 0.2136891\ttest: 0.2303470\tbest: 0.2303470 (200)\ttotal: 3.37s\tremaining: 1m 26s\n",
      "220:\tlearn: 0.2118267\ttest: 0.2298797\tbest: 0.2298524 (219)\ttotal: 3.7s\tremaining: 1m 26s\n",
      "240:\tlearn: 0.2102121\ttest: 0.2293902\tbest: 0.2293902 (240)\ttotal: 4.02s\tremaining: 1m 25s\n",
      "260:\tlearn: 0.2087311\ttest: 0.2291016\tbest: 0.2291016 (260)\ttotal: 4.34s\tremaining: 1m 25s\n",
      "280:\tlearn: 0.2071437\ttest: 0.2288199\tbest: 0.2288199 (280)\ttotal: 4.67s\tremaining: 1m 24s\n",
      "300:\tlearn: 0.2057581\ttest: 0.2285476\tbest: 0.2285476 (300)\ttotal: 4.99s\tremaining: 1m 24s\n",
      "320:\tlearn: 0.2041938\ttest: 0.2283450\tbest: 0.2283450 (320)\ttotal: 5.31s\tremaining: 1m 23s\n",
      "340:\tlearn: 0.2029008\ttest: 0.2281580\tbest: 0.2281580 (340)\ttotal: 5.63s\tremaining: 1m 23s\n",
      "360:\tlearn: 0.2015661\ttest: 0.2279911\tbest: 0.2279911 (360)\ttotal: 5.95s\tremaining: 1m 22s\n",
      "380:\tlearn: 0.2001865\ttest: 0.2277613\tbest: 0.2277490 (379)\ttotal: 6.27s\tremaining: 1m 22s\n",
      "400:\tlearn: 0.1989161\ttest: 0.2275505\tbest: 0.2275505 (400)\ttotal: 6.6s\tremaining: 1m 21s\n",
      "420:\tlearn: 0.1975075\ttest: 0.2273430\tbest: 0.2273430 (420)\ttotal: 6.92s\tremaining: 1m 21s\n",
      "440:\tlearn: 0.1963189\ttest: 0.2272923\tbest: 0.2272520 (436)\ttotal: 7.24s\tremaining: 1m 21s\n",
      "460:\tlearn: 0.1952191\ttest: 0.2272288\tbest: 0.2272139 (459)\ttotal: 7.55s\tremaining: 1m 20s\n",
      "480:\tlearn: 0.1938861\ttest: 0.2269524\tbest: 0.2269524 (480)\ttotal: 7.87s\tremaining: 1m 20s\n",
      "500:\tlearn: 0.1927516\ttest: 0.2269582\tbest: 0.2269524 (480)\ttotal: 8.19s\tremaining: 1m 19s\n",
      "520:\tlearn: 0.1916906\ttest: 0.2268268\tbest: 0.2268185 (518)\ttotal: 8.49s\tremaining: 1m 19s\n",
      "540:\tlearn: 0.1906672\ttest: 0.2268165\tbest: 0.2268165 (540)\ttotal: 8.8s\tremaining: 1m 18s\n",
      "560:\tlearn: 0.1894573\ttest: 0.2266901\tbest: 0.2266747 (557)\ttotal: 9.13s\tremaining: 1m 18s\n",
      "580:\tlearn: 0.1883504\ttest: 0.2265022\tbest: 0.2265022 (580)\ttotal: 9.46s\tremaining: 1m 18s\n",
      "600:\tlearn: 0.1873003\ttest: 0.2264680\tbest: 0.2264484 (593)\ttotal: 9.78s\tremaining: 1m 17s\n",
      "620:\tlearn: 0.1862771\ttest: 0.2264445\tbest: 0.2264197 (616)\ttotal: 10.1s\tremaining: 1m 17s\n",
      "640:\tlearn: 0.1852003\ttest: 0.2263810\tbest: 0.2263547 (637)\ttotal: 10.4s\tremaining: 1m 17s\n",
      "660:\tlearn: 0.1841481\ttest: 0.2263387\tbest: 0.2263387 (660)\ttotal: 10.8s\tremaining: 1m 16s\n",
      "680:\tlearn: 0.1830620\ttest: 0.2262974\tbest: 0.2262756 (674)\ttotal: 11.1s\tremaining: 1m 16s\n",
      "700:\tlearn: 0.1818942\ttest: 0.2262158\tbest: 0.2262158 (700)\ttotal: 11.4s\tremaining: 1m 16s\n",
      "720:\tlearn: 0.1809612\ttest: 0.2260835\tbest: 0.2260835 (720)\ttotal: 11.8s\tremaining: 1m 16s\n",
      "740:\tlearn: 0.1799388\ttest: 0.2259850\tbest: 0.2259724 (739)\ttotal: 12.1s\tremaining: 1m 15s\n",
      "760:\tlearn: 0.1788726\ttest: 0.2260126\tbest: 0.2259724 (739)\ttotal: 12.4s\tremaining: 1m 15s\n",
      "780:\tlearn: 0.1778052\ttest: 0.2259792\tbest: 0.2259388 (773)\ttotal: 12.7s\tremaining: 1m 14s\n",
      "800:\tlearn: 0.1768481\ttest: 0.2258916\tbest: 0.2258365 (796)\ttotal: 13.1s\tremaining: 1m 14s\n",
      "820:\tlearn: 0.1758194\ttest: 0.2258945\tbest: 0.2258365 (796)\ttotal: 13.4s\tremaining: 1m 14s\n",
      "840:\tlearn: 0.1747095\ttest: 0.2258479\tbest: 0.2258365 (796)\ttotal: 13.7s\tremaining: 1m 14s\n",
      "860:\tlearn: 0.1736996\ttest: 0.2257353\tbest: 0.2257353 (860)\ttotal: 14s\tremaining: 1m 13s\n",
      "880:\tlearn: 0.1727339\ttest: 0.2257513\tbest: 0.2257075 (861)\ttotal: 14.4s\tremaining: 1m 13s\n",
      "900:\tlearn: 0.1717318\ttest: 0.2257168\tbest: 0.2257075 (861)\ttotal: 14.7s\tremaining: 1m 13s\n",
      "920:\tlearn: 0.1707965\ttest: 0.2256407\tbest: 0.2256206 (919)\ttotal: 15s\tremaining: 1m 12s\n",
      "940:\tlearn: 0.1699078\ttest: 0.2255118\tbest: 0.2255001 (939)\ttotal: 15.3s\tremaining: 1m 12s\n",
      "960:\tlearn: 0.1690430\ttest: 0.2255198\tbest: 0.2254748 (946)\ttotal: 15.7s\tremaining: 1m 12s\n",
      "980:\tlearn: 0.1680783\ttest: 0.2254225\tbest: 0.2254225 (980)\ttotal: 16s\tremaining: 1m 11s\n",
      "1000:\tlearn: 0.1672367\ttest: 0.2253632\tbest: 0.2253632 (1000)\ttotal: 16.3s\tremaining: 1m 11s\n",
      "1020:\tlearn: 0.1663719\ttest: 0.2253465\tbest: 0.2253211 (1017)\ttotal: 16.6s\tremaining: 1m 11s\n",
      "1040:\tlearn: 0.1654510\ttest: 0.2252195\tbest: 0.2252195 (1040)\ttotal: 17s\tremaining: 1m 10s\n",
      "1060:\tlearn: 0.1645521\ttest: 0.2252238\tbest: 0.2251822 (1049)\ttotal: 17.3s\tremaining: 1m 10s\n",
      "1080:\tlearn: 0.1637037\ttest: 0.2251774\tbest: 0.2251774 (1080)\ttotal: 17.6s\tremaining: 1m 10s\n",
      "1100:\tlearn: 0.1628914\ttest: 0.2251320\tbest: 0.2251319 (1095)\ttotal: 17.9s\tremaining: 1m 9s\n",
      "1120:\tlearn: 0.1621046\ttest: 0.2251038\tbest: 0.2250849 (1112)\ttotal: 18.3s\tremaining: 1m 9s\n",
      "1140:\tlearn: 0.1613185\ttest: 0.2251266\tbest: 0.2250526 (1126)\ttotal: 18.7s\tremaining: 1m 9s\n",
      "1160:\tlearn: 0.1604662\ttest: 0.2251856\tbest: 0.2250526 (1126)\ttotal: 19s\tremaining: 1m 9s\n",
      "bestTest = 0.2250526247\n",
      "bestIteration = 1126\n",
      "Shrink model to first 1127 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "\t-0.2236\t = Validation score   (-log_loss)\n",
      "\t153.69s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 2485.43s of the 2485.41s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\ExtraTreesGini_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\ExtraTreesGini_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 12\n",
      "C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "\tWarning: Exception caused ExtraTreesGini_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput contains NaN, infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 220, in _fit\n",
      "    self._fit_single(X=X, y=y, model_base=model_base, use_child_oof=use_child_oof, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 326, in _fit_single\n",
      "    model_base.fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 189, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 327, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 2480.01s of the 2479.99s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\ExtraTreesEntr_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\ExtraTreesEntr_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 12\n",
      "C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "\tWarning: Exception caused ExtraTreesEntr_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput contains NaN, infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 220, in _fit\n",
      "    self._fit_single(X=X, y=y, model_base=model_base, use_child_oof=use_child_oof, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 326, in _fit_single\n",
      "    model_base.fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\rf\\rf_model.py\", line 189, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 327, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2474.63s of the 2474.61s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 4.84 GB, but only 6.776 GB is available...\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 0/0 categorical features\n",
      "Using 1459 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(1459, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=1459, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 198.03 / 203.28 sec\n",
      "No improvement since epoch -1: early stopping\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t[Errno 2] No such file or directory: 'C:\\\\Users\\\\Kaleb\\\\AppData\\\\Local\\\\Temp\\\\tmplfolf9l8\\\\models\\\\model.pth'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 217, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 222, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 252, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **kwargs_fold)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 287, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params['lr'], cbs=callbacks)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\callback\\schedule.py\", line 122, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\learner.py\", line 242, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\learner.py\", line 181, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\learner.py\", line 157, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastcore\\foundation.py\", line 156, in map\n",
      "    def map(self, f, *args, gen=False, **kwargs): return self._new(map_ex(self, f, *args, gen=gen, **kwargs))\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastcore\\basics.py\", line 814, in map_ex\n",
      "    return list(res)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastcore\\basics.py\", line 799, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\learner.py\", line 161, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\callback\\core.py\", line 65, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\callback\\core.py\", line 63, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\callbacks.py\", line 107, in after_fit\n",
      "    self.learn.load(f'{self.fname}', with_opt=self.with_opt)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\learner.py\", line 387, in load\n",
      "    load_model(file, self.model, self.opt, device=device, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\fastai\\learner.py\", line 51, in load_model\n",
      "    state = torch.load(file, map_location=device)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\serialization.py\", line 699, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\serialization.py\", line 230, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\serialization.py\", line 211, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Kaleb\\\\AppData\\\\Local\\\\Temp\\\\tmplfolf9l8\\\\models\\\\model.pth'\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2310.3s of the 2310.28s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train XGBoost model, roughly requires: 3.63 GB, but only 3.997 GB is available...\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t[21:58:52] C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/data/data.cc:945: Check failed: valid: Input data contains `inf` or `nan`\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 217, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 222, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 252, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **kwargs_fold)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 136, in _fit\n",
      "    self.model.fit(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\sklearn.py\", line 1158, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\sklearn.py\", line 1172, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\core.py\", line 541, in __init__\n",
      "    handle, feature_names, feature_types = dispatch_data_backend(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\data.py\", line 558, in dispatch_data_backend\n",
      "    return _from_scipy_csr(data, missing, threads, feature_names, feature_types)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\data.py\", line 55, in _from_scipy_csr\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\core.py\", line 210, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [21:58:52] C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/data/data.cc:945: Check failed: valid: Input data contains `inf` or `nan`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2297.69s of the 2297.67s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tMemory not enough to fit TabularNeuralNetTorchModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"P_2_first\",\n",
      "        \"P_2_mean\",\n",
      "        \"P_2_min\",\n",
      "        \"P_2_max\",\n",
      "        \"P_2_last\",\n",
      "        \"B_2_first\",\n",
      "        \"B_2_mean\",\n",
      "        \"B_2_std\",\n",
      "        \"B_2_min\",\n",
      "        \"B_2_last\",\n",
      "        \"D_46_first\",\n",
      "        \"D_46_last\",\n",
      "        \"D_47_first\",\n",
      "        \"D_47_mean\",\n",
      "        \"D_47_min\",\n",
      "        \"D_47_max\",\n",
      "        \"D_47_last\",\n",
      "        \"D_48_first\",\n",
      "        \"D_48_mean\",\n",
      "        \"D_48_max\",\n",
      "        \"D_48_last\",\n",
      "        \"B_8_first\",\n",
      "        \"B_8_mean\",\n",
      "        \"B_8_min\",\n",
      "        \"B_8_max\",\n",
      "        \"B_8_last\",\n",
      "        \"S_8_first\",\n",
      "        \"S_8_mean\",\n",
      "        \"S_8_std\",\n",
      "        \"S_8_max\",\n",
      "        \"S_8_last\",\n",
      "        \"D_55_mean\",\n",
      "        \"D_55_max\",\n",
      "        \"D_55_last\",\n",
      "        \"D_58_max\",\n",
      "        \"D_59_first\",\n",
      "        \"D_59_mean\",\n",
      "        \"D_59_min\",\n",
      "        \"D_59_max\",\n",
      "        \"D_59_last\",\n",
      "        \"D_60_first\",\n",
      "        \"D_60_mean\",\n",
      "        \"D_60_max\",\n",
      "        \"D_60_last\",\n",
      "        \"D_61_last\",\n",
      "        \"S_11_min\",\n",
      "        \"B_16_first\",\n",
      "        \"B_16_mean\",\n",
      "        \"B_16_max\",\n",
      "        \"B_16_last\",\n",
      "        \"B_17_mean\",\n",
      "        \"B_17_std\",\n",
      "        \"B_17_min\",\n",
      "        \"B_17_last\",\n",
      "        \"B_18_first\",\n",
      "        \"B_18_mean\",\n",
      "        \"B_18_std\",\n",
      "        \"B_18_min\",\n",
      "        \"B_18_max\",\n",
      "        \"B_18_last\",\n",
      "        \"B_20_max\",\n",
      "        \"S_12_min\",\n",
      "        \"S_13_first\",\n",
      "        \"S_13_std\",\n",
      "        \"S_13_max\",\n",
      "        \"S_13_last\",\n",
      "        \"S_15_mean\",\n",
      "        \"S_15_min\",\n",
      "        \"B_27_first\",\n",
      "        \"B_27_mean\",\n",
      "        \"D_81_min\",\n",
      "        \"D_83_min\",\n",
      "        \"S_19_last\",\n",
      "        \"B_33_first\",\n",
      "        \"B_33_mean\",\n",
      "        \"B_33_std\",\n",
      "        \"B_33_min\",\n",
      "        \"B_33_last\",\n",
      "        \"D_91_first\",\n",
      "        \"D_91_min\",\n",
      "        \"D_103_first\",\n",
      "        \"D_103_mean\",\n",
      "        \"D_103_min\",\n",
      "        \"D_103_max\",\n",
      "        \"D_103_last\",\n",
      "        \"D_104_first\",\n",
      "        \"D_104_mean\",\n",
      "        \"D_104_min\",\n",
      "        \"D_104_max\",\n",
      "        \"D_104_last\",\n",
      "        \"D_105_first\",\n",
      "        \"D_105_mean\",\n",
      "        \"D_105_max\",\n",
      "        \"D_105_last\",\n",
      "        \"D_107_first\",\n",
      "        \"D_107_min\",\n",
      "        \"D_109_first\",\n",
      "        \"D_110_first\",\n",
      "        \"D_110_mean\",\n",
      "        \"D_110_min\",\n",
      "        \"D_110_last\",\n",
      "        \"B_39_max\",\n",
      "        \"D_112_min\",\n",
      "        \"D_118_max\",\n",
      "        \"D_119_max\",\n",
      "        \"D_121_first\",\n",
      "        \"D_121_mean\",\n",
      "        \"D_121_min\",\n",
      "        \"D_121_max\",\n",
      "        \"D_121_last\",\n",
      "        \"D_122_first\",\n",
      "        \"D_122_mean\",\n",
      "        \"D_122_min\",\n",
      "        \"D_122_max\",\n",
      "        \"D_122_last\",\n",
      "        \"D_123_first\",\n",
      "        \"D_124_first\",\n",
      "        \"D_124_min\",\n",
      "        \"D_125_first\",\n",
      "        \"D_125_mean\",\n",
      "        \"D_128_first\",\n",
      "        \"D_128_mean\",\n",
      "        \"D_128_min\",\n",
      "        \"D_128_max\",\n",
      "        \"D_128_last\",\n",
      "        \"D_129_first\",\n",
      "        \"D_129_mean\",\n",
      "        \"D_129_min\",\n",
      "        \"D_129_max\",\n",
      "        \"D_129_last\",\n",
      "        \"D_134_max\",\n",
      "        \"D_139_first\",\n",
      "        \"D_139_min\",\n",
      "        \"D_142_first\",\n",
      "        \"D_142_mean\",\n",
      "        \"D_142_min\",\n",
      "        \"D_142_max\",\n",
      "        \"D_142_last\",\n",
      "        \"D_143_first\",\n",
      "        \"D_143_min\",\n",
      "        \"D_39_last_lag_div\",\n",
      "        \"B_2_last_lag_sub\",\n",
      "        \"S_3_last_lag_sub\",\n",
      "        \"D_41_last_lag_div\",\n",
      "        \"D_44_last_lag_div\",\n",
      "        \"B_4_last_lag_div\",\n",
      "        \"R_2_last_lag_div\",\n",
      "        \"D_46_last_lag_sub\",\n",
      "        \"D_47_last_lag_sub\",\n",
      "        \"D_48_last_lag_sub\",\n",
      "        \"D_49_last_lag_div\",\n",
      "        \"B_7_last_lag_sub\",\n",
      "        \"B_8_last_lag_sub\",\n",
      "        \"B_8_last_lag_div\",\n",
      "        \"D_51_last_lag_sub\",\n",
      "        \"D_51_last_lag_div\",\n",
      "        \"R_3_last_lag_div\",\n",
      "        \"D_52_last_lag_sub\",\n",
      "        \"P_3_last_lag_sub\",\n",
      "        \"S_6_last_lag_sub\",\n",
      "        \"S_6_last_lag_div\",\n",
      "        \"R_4_last_lag_div\",\n",
      "        \"S_7_last_lag_sub\",\n",
      "        \"B_12_last_lag_sub\",\n",
      "        \"S_8_last_lag_sub\",\n",
      "        \"S_8_last_lag_div\",\n",
      "        \"D_55_last_lag_sub\",\n",
      "        \"D_56_last_lag_sub\",\n",
      "        \"R_5_last_lag_div\",\n",
      "        \"D_58_last_lag_sub\",\n",
      "        \"D_60_last_lag_sub\",\n",
      "        \"S_11_last_lag_sub\",\n",
      "        \"S_11_last_lag_div\",\n",
      "        \"D_65_last_lag_div\",\n",
      "        \"B_16_last_lag_sub\",\n",
      "        \"B_16_last_lag_div\",\n",
      "        \"B_17_last_lag_sub\",\n",
      "        \"B_18_last_lag_sub\",\n",
      "        \"B_19_last_lag_div\",\n",
      "        \"B_20_last_lag_sub\",\n",
      "        \"B_20_last_lag_div\",\n",
      "        \"S_13_last_lag_sub\",\n",
      "        \"S_13_last_lag_div\",\n",
      "        \"B_22_last_lag_sub\",\n",
      "        \"B_22_last_lag_div\",\n",
      "        \"D_70_last_lag_div\",\n",
      "        \"D_72_last_lag_div\",\n",
      "        \"S_15_last_lag_sub\",\n",
      "        \"S_15_last_lag_div\",\n",
      "        \"B_23_last_lag_sub\",\n",
      "        \"P_4_last_lag_div\",\n",
      "        \"D_74_last_lag_div\",\n",
      "        \"D_75_last_lag_div\",\n",
      "        \"R_7_last_lag_div\",\n",
      "        \"D_78_last_lag_div\",\n",
      "        \"D_79_last_lag_div\",\n",
      "        \"R_8_last_lag_div\",\n",
      "        \"R_9_last_lag_div\",\n",
      "        \"D_80_last_lag_sub\",\n",
      "        \"D_80_last_lag_div\",\n",
      "        \"R_10_last_lag_div\",\n",
      "        \"R_11_last_lag_sub\",\n",
      "        \"R_11_last_lag_div\",\n",
      "        \"B_27_last_lag_sub\",\n",
      "        \"D_81_last_lag_div\",\n",
      "        \"D_82_last_lag_div\",\n",
      "        \"S_17_last_lag_sub\",\n",
      "        \"R_13_last_lag_div\",\n",
      "        \"D_83_last_lag_sub\",\n",
      "        \"D_83_last_lag_div\",\n",
      "        \"R_14_last_lag_div\",\n",
      "        \"R_15_last_lag_div\",\n",
      "        \"D_84_last_lag_div\",\n",
      "        \"R_16_last_lag_div\",\n",
      "        \"S_18_last_lag_div\",\n",
      "        \"D_86_last_lag_sub\",\n",
      "        \"D_86_last_lag_div\",\n",
      "        \"R_17_last_lag_div\",\n",
      "        \"R_18_last_lag_div\",\n",
      "        \"B_31_last_lag_div\",\n",
      "        \"S_19_last_lag_sub\",\n",
      "        \"R_19_last_lag_div\",\n",
      "        \"B_32_last_lag_div\",\n",
      "        \"S_20_last_lag_sub\",\n",
      "        \"S_20_last_lag_div\",\n",
      "        \"R_20_last_lag_div\",\n",
      "        \"R_21_last_lag_sub\",\n",
      "        \"R_21_last_lag_div\",\n",
      "        \"B_33_last_lag_sub\",\n",
      "        \"B_33_last_lag_div\",\n",
      "        \"D_89_last_lag_div\",\n",
      "        \"R_22_last_lag_div\",\n",
      "        \"R_23_last_lag_div\",\n",
      "        \"D_91_last_lag_sub\",\n",
      "        \"D_91_last_lag_div\",\n",
      "        \"D_92_last_lag_sub\",\n",
      "        \"D_92_last_lag_div\",\n",
      "        \"D_93_last_lag_div\",\n",
      "        \"D_94_last_lag_sub\",\n",
      "        \"D_94_last_lag_div\",\n",
      "        \"R_24_last_lag_div\",\n",
      "        \"R_25_last_lag_div\",\n",
      "        \"D_96_last_lag_sub\",\n",
      "        \"D_96_last_lag_div\",\n",
      "        \"D_103_last_lag_div\",\n",
      "        \"D_104_last_lag_sub\",\n",
      "        \"D_104_last_lag_div\",\n",
      "        \"D_106_last_lag_sub\",\n",
      "        \"D_106_last_lag_div\",\n",
      "        \"D_107_last_lag_div\",\n",
      "        \"R_26_last_lag_div\",\n",
      "        \"D_108_last_lag_div\",\n",
      "        \"D_109_last_lag_div\",\n",
      "        \"D_111_last_lag_div\",\n",
      "        \"B_39_last_lag_sub\",\n",
      "        \"D_112_last_lag_sub\",\n",
      "        \"D_113_last_lag_div\",\n",
      "        \"D_115_last_lag_sub\",\n",
      "        \"D_118_last_lag_sub\",\n",
      "        \"D_119_last_lag_sub\",\n",
      "        \"D_122_last_lag_div\",\n",
      "        \"D_123_last_lag_div\",\n",
      "        \"D_124_last_lag_div\",\n",
      "        \"D_125_last_lag_sub\",\n",
      "        \"D_125_last_lag_div\",\n",
      "        \"D_127_last_lag_sub\",\n",
      "        \"D_127_last_lag_div\",\n",
      "        \"D_128_last_lag_div\",\n",
      "        \"D_129_last_lag_div\",\n",
      "        \"B_41_last_lag_div\",\n",
      "        \"D_130_last_lag_div\",\n",
      "        \"D_131_last_lag_div\",\n",
      "        \"R_28_last_lag_div\",\n",
      "        \"D_135_last_lag_sub\",\n",
      "        \"D_135_last_lag_div\",\n",
      "        \"D_136_last_lag_div\",\n",
      "        \"D_137_last_lag_sub\",\n",
      "        \"D_137_last_lag_div\",\n",
      "        \"D_138_last_lag_sub\",\n",
      "        \"D_138_last_lag_div\",\n",
      "        \"D_139_last_lag_div\",\n",
      "        \"D_140_last_lag_div\",\n",
      "        \"D_141_last_lag_div\",\n",
      "        \"D_143_last_lag_div\",\n",
      "        \"D_145_last_lag_div\",\n",
      "        \"B_38_nunique\",\n",
      "        \"D_114_first\",\n",
      "        \"D_114_last\",\n",
      "        \"D_117_first\",\n",
      "        \"D_117_last\",\n",
      "        \"D_117_nunique\",\n",
      "        \"D_120_first\",\n",
      "        \"D_64_first\",\n",
      "        \"D_64_last\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"P_2_std\",\n",
      "        \"D_39_first\",\n",
      "        \"D_39_mean\",\n",
      "        \"D_39_std\",\n",
      "        \"D_39_min\",\n",
      "        \"D_39_max\",\n",
      "        \"D_39_last\",\n",
      "        \"B_1_first\",\n",
      "        \"B_1_mean\",\n",
      "        \"B_1_std\",\n",
      "        \"B_1_min\",\n",
      "        \"B_1_max\",\n",
      "        \"B_1_last\",\n",
      "        \"B_2_max\",\n",
      "        \"R_1_first\",\n",
      "        \"R_1_mean\",\n",
      "        \"R_1_std\",\n",
      "        \"R_1_min\",\n",
      "        \"R_1_max\",\n",
      "        \"R_1_last\",\n",
      "        \"S_3_first\",\n",
      "        \"S_3_mean\",\n",
      "        \"S_3_std\",\n",
      "        \"S_3_min\",\n",
      "        \"S_3_max\",\n",
      "        \"S_3_last\",\n",
      "        \"D_41_first\",\n",
      "        \"D_41_mean\",\n",
      "        \"D_41_std\",\n",
      "        \"D_41_min\",\n",
      "        \"D_41_max\",\n",
      "        \"D_41_last\",\n",
      "        \"B_3_first\",\n",
      "        \"B_3_mean\",\n",
      "        \"B_3_std\",\n",
      "        \"B_3_min\",\n",
      "        \"B_3_max\",\n",
      "        \"B_3_last\",\n",
      "        \"D_42_first\",\n",
      "        \"D_42_mean\",\n",
      "        \"D_42_std\",\n",
      "        \"D_42_min\",\n",
      "        \"D_42_max\",\n",
      "        \"D_42_last\",\n",
      "        \"D_43_first\",\n",
      "        \"D_43_mean\",\n",
      "        \"D_43_std\",\n",
      "        \"D_43_min\",\n",
      "        \"D_43_max\",\n",
      "        \"D_43_last\",\n",
      "        \"D_44_first\",\n",
      "        \"D_44_mean\",\n",
      "        \"D_44_std\",\n",
      "        \"D_44_min\",\n",
      "        \"D_44_max\",\n",
      "        \"D_44_last\",\n",
      "        \"B_4_first\",\n",
      "        \"B_4_mean\",\n",
      "        \"B_4_std\",\n",
      "        \"B_4_min\",\n",
      "        \"B_4_max\",\n",
      "        \"B_4_last\",\n",
      "        \"D_45_first\",\n",
      "        \"D_45_mean\",\n",
      "        \"D_45_std\",\n",
      "        \"D_45_min\",\n",
      "        \"D_45_max\",\n",
      "        \"D_45_last\",\n",
      "        \"B_5_first\",\n",
      "        \"B_5_mean\",\n",
      "        \"B_5_std\",\n",
      "        \"B_5_min\",\n",
      "        \"B_5_max\",\n",
      "        \"B_5_last\",\n",
      "        \"R_2_mean\",\n",
      "        \"R_2_std\",\n",
      "        \"D_46_mean\",\n",
      "        \"D_46_std\",\n",
      "        \"D_46_min\",\n",
      "        \"D_46_max\",\n",
      "        \"D_47_std\",\n",
      "        \"D_48_std\",\n",
      "        \"D_48_min\",\n",
      "        \"D_49_first\",\n",
      "        \"D_49_mean\",\n",
      "        \"D_49_std\",\n",
      "        \"D_49_min\",\n",
      "        \"D_49_max\",\n",
      "        \"D_49_last\",\n",
      "        \"B_6_first\",\n",
      "        \"B_6_mean\",\n",
      "        \"B_6_std\",\n",
      "        \"B_6_min\",\n",
      "        \"B_6_max\",\n",
      "        \"B_6_last\",\n",
      "        \"B_7_first\",\n",
      "        \"B_7_mean\",\n",
      "        \"B_7_std\",\n",
      "        \"B_7_min\",\n",
      "        \"B_7_max\",\n",
      "        \"B_7_last\",\n",
      "        \"B_8_std\",\n",
      "        \"D_50_first\",\n",
      "        \"D_50_mean\",\n",
      "        \"D_50_std\",\n",
      "        \"D_50_min\",\n",
      "        \"D_50_max\",\n",
      "        \"D_50_last\",\n",
      "        \"D_51_first\",\n",
      "        \"D_51_mean\",\n",
      "        \"D_51_std\",\n",
      "        \"D_51_min\",\n",
      "        \"D_51_max\",\n",
      "        \"D_51_last\",\n",
      "        \"B_9_first\",\n",
      "        \"B_9_mean\",\n",
      "        \"B_9_std\",\n",
      "        \"B_9_min\",\n",
      "        \"B_9_max\",\n",
      "        \"B_9_last\",\n",
      "        \"R_3_first\",\n",
      "        \"R_3_mean\",\n",
      "        \"R_3_std\",\n",
      "        \"R_3_min\",\n",
      "        \"R_3_max\",\n",
      "        \"R_3_last\",\n",
      "        \"D_52_first\",\n",
      "        \"D_52_mean\",\n",
      "        \"D_52_std\",\n",
      "        \"D_52_min\",\n",
      "        \"D_52_max\",\n",
      "        \"D_52_last\",\n",
      "        \"P_3_first\",\n",
      "        \"P_3_mean\",\n",
      "        \"P_3_std\",\n",
      "        \"P_3_min\",\n",
      "        \"P_3_max\",\n",
      "        \"P_3_last\",\n",
      "        \"B_10_first\",\n",
      "        \"B_10_mean\",\n",
      "        \"B_10_std\",\n",
      "        \"B_10_min\",\n",
      "        \"B_10_max\",\n",
      "        \"B_10_last\",\n",
      "        \"D_53_first\",\n",
      "        \"D_53_mean\",\n",
      "        \"D_53_std\",\n",
      "        \"D_53_min\",\n",
      "        \"D_53_max\",\n",
      "        \"D_53_last\",\n",
      "        \"S_5_first\",\n",
      "        \"S_5_mean\",\n",
      "        \"S_5_std\",\n",
      "        \"S_5_min\",\n",
      "        \"S_5_max\",\n",
      "        \"S_5_last\",\n",
      "        \"B_11_first\",\n",
      "        \"B_11_mean\",\n",
      "        \"B_11_std\",\n",
      "        \"B_11_min\",\n",
      "        \"B_11_max\",\n",
      "        \"B_11_last\",\n",
      "        \"S_6_mean\",\n",
      "        \"S_6_std\",\n",
      "        \"D_54_first\",\n",
      "        \"D_54_mean\",\n",
      "        \"D_54_std\",\n",
      "        \"D_54_min\",\n",
      "        \"D_54_max\",\n",
      "        \"D_54_last\",\n",
      "        \"R_4_mean\",\n",
      "        \"R_4_std\",\n",
      "        \"S_7_first\",\n",
      "        \"S_7_mean\",\n",
      "        \"S_7_std\",\n",
      "        \"S_7_min\",\n",
      "        \"S_7_max\",\n",
      "        \"S_7_last\",\n",
      "        \"B_12_first\",\n",
      "        \"B_12_mean\",\n",
      "        \"B_12_std\",\n",
      "        \"B_12_min\",\n",
      "        \"B_12_max\",\n",
      "        \"B_12_last\",\n",
      "        \"S_8_min\",\n",
      "        \"D_55_first\",\n",
      "        \"D_55_std\",\n",
      "        \"D_55_min\",\n",
      "        \"D_56_first\",\n",
      "        \"D_56_mean\",\n",
      "        \"D_56_std\",\n",
      "        \"D_56_min\",\n",
      "        \"D_56_max\",\n",
      "        \"D_56_last\",\n",
      "        \"B_13_first\",\n",
      "        \"B_13_mean\",\n",
      "        \"B_13_std\",\n",
      "        \"B_13_min\",\n",
      "        \"B_13_max\",\n",
      "        \"B_13_last\",\n",
      "        \"R_5_first\",\n",
      "        \"R_5_mean\",\n",
      "        \"R_5_std\",\n",
      "        \"R_5_min\",\n",
      "        \"R_5_max\",\n",
      "        \"R_5_last\",\n",
      "        \"D_58_first\",\n",
      "        \"D_58_mean\",\n",
      "        \"D_58_std\",\n",
      "        \"D_58_min\",\n",
      "        \"D_58_last\",\n",
      "        \"S_9_first\",\n",
      "        \"S_9_mean\",\n",
      "        \"S_9_std\",\n",
      "        \"S_9_min\",\n",
      "        \"S_9_max\",\n",
      "        \"S_9_last\",\n",
      "        \"B_14_first\",\n",
      "        \"B_14_mean\",\n",
      "        \"B_14_std\",\n",
      "        \"B_14_min\",\n",
      "        \"B_14_max\",\n",
      "        \"B_14_last\",\n",
      "        \"D_59_std\",\n",
      "        \"D_60_std\",\n",
      "        \"D_60_min\",\n",
      "        \"D_61_first\",\n",
      "        \"D_61_mean\",\n",
      "        \"D_61_std\",\n",
      "        \"D_61_min\",\n",
      "        \"D_61_max\",\n",
      "        \"B_15_first\",\n",
      "        \"B_15_mean\",\n",
      "        \"B_15_std\",\n",
      "        \"B_15_min\",\n",
      "        \"B_15_max\",\n",
      "        \"B_15_last\",\n",
      "        \"S_11_first\",\n",
      "        \"S_11_mean\",\n",
      "        \"S_11_std\",\n",
      "        \"S_11_max\",\n",
      "        \"S_11_last\",\n",
      "        \"D_62_first\",\n",
      "        \"D_62_mean\",\n",
      "        \"D_62_std\",\n",
      "        \"D_62_min\",\n",
      "        \"D_62_max\",\n",
      "        \"D_62_last\",\n",
      "        \"D_65_first\",\n",
      "        \"D_65_mean\",\n",
      "        \"D_65_std\",\n",
      "        \"D_65_min\",\n",
      "        \"D_65_max\",\n",
      "        \"D_65_last\",\n",
      "        \"B_16_std\",\n",
      "        \"B_16_min\",\n",
      "        \"B_17_first\",\n",
      "        \"B_17_max\",\n",
      "        \"B_19_first\",\n",
      "        \"B_19_mean\",\n",
      "        \"B_19_std\",\n",
      "        \"B_19_min\",\n",
      "        \"B_19_max\",\n",
      "        \"B_19_last\",\n",
      "        \"B_20_first\",\n",
      "        \"B_20_mean\",\n",
      "        \"B_20_std\",\n",
      "        \"B_20_min\",\n",
      "        \"B_20_last\",\n",
      "        \"S_12_first\",\n",
      "        \"S_12_mean\",\n",
      "        \"S_12_std\",\n",
      "        \"S_12_max\",\n",
      "        \"S_12_last\",\n",
      "        \"R_6_first\",\n",
      "        \"R_6_mean\",\n",
      "        \"R_6_std\",\n",
      "        \"R_6_min\",\n",
      "        \"R_6_max\",\n",
      "        \"R_6_last\",\n",
      "        \"S_13_mean\",\n",
      "        \"S_13_min\",\n",
      "        \"B_21_first\",\n",
      "        \"B_21_mean\",\n",
      "        \"B_21_std\",\n",
      "        \"B_21_min\",\n",
      "        \"B_21_max\",\n",
      "        \"B_21_last\",\n",
      "        \"D_69_first\",\n",
      "        \"D_69_mean\",\n",
      "        \"D_69_std\",\n",
      "        \"D_69_min\",\n",
      "        \"D_69_max\",\n",
      "        \"D_69_last\",\n",
      "        \"B_22_first\",\n",
      "        \"B_22_mean\",\n",
      "        \"B_22_std\",\n",
      "        \"B_22_min\",\n",
      "        \"B_22_max\",\n",
      "        \"B_22_last\",\n",
      "        \"D_70_first\",\n",
      "        \"D_70_mean\",\n",
      "        \"D_70_std\",\n",
      "        \"D_70_min\",\n",
      "        \"D_70_max\",\n",
      "        \"D_70_last\",\n",
      "        \"D_71_first\",\n",
      "        \"D_71_mean\",\n",
      "        \"D_71_std\",\n",
      "        \"D_71_min\",\n",
      "        \"D_71_max\",\n",
      "        \"D_71_last\",\n",
      "        \"D_72_first\",\n",
      "        \"D_72_mean\",\n",
      "        \"D_72_std\",\n",
      "        \"D_72_min\",\n",
      "        \"D_72_max\",\n",
      "        \"D_72_last\",\n",
      "        \"S_15_first\",\n",
      "        \"S_15_std\",\n",
      "        \"S_15_max\",\n",
      "        \"S_15_last\",\n",
      "        \"B_23_first\",\n",
      "        \"B_23_mean\",\n",
      "        \"B_23_std\",\n",
      "        \"B_23_min\",\n",
      "        \"B_23_max\",\n",
      "        \"B_23_last\",\n",
      "        \"D_73_first\",\n",
      "        \"D_73_mean\",\n",
      "        \"D_73_std\",\n",
      "        \"D_73_min\",\n",
      "        \"D_73_max\",\n",
      "        \"D_73_last\",\n",
      "        \"P_4_first\",\n",
      "        \"P_4_mean\",\n",
      "        \"P_4_std\",\n",
      "        \"P_4_min\",\n",
      "        \"P_4_max\",\n",
      "        \"P_4_last\",\n",
      "        \"D_74_first\",\n",
      "        \"D_74_mean\",\n",
      "        \"D_74_std\",\n",
      "        \"D_74_min\",\n",
      "        \"D_74_max\",\n",
      "        \"D_74_last\",\n",
      "        \"D_75_first\",\n",
      "        \"D_75_mean\",\n",
      "        \"D_75_std\",\n",
      "        \"D_75_min\",\n",
      "        \"D_75_max\",\n",
      "        \"D_75_last\",\n",
      "        \"D_76_first\",\n",
      "        \"D_76_mean\",\n",
      "        \"D_76_std\",\n",
      "        \"D_76_min\",\n",
      "        \"D_76_max\",\n",
      "        \"D_76_last\",\n",
      "        \"B_24_first\",\n",
      "        \"B_24_mean\",\n",
      "        \"B_24_std\",\n",
      "        \"B_24_min\",\n",
      "        \"B_24_max\",\n",
      "        \"B_24_last\",\n",
      "        \"R_7_first\",\n",
      "        \"R_7_mean\",\n",
      "        \"R_7_std\",\n",
      "        \"R_7_min\",\n",
      "        \"R_7_max\",\n",
      "        \"R_7_last\",\n",
      "        \"D_77_first\",\n",
      "        \"D_77_mean\",\n",
      "        \"D_77_std\",\n",
      "        \"D_77_min\",\n",
      "        \"D_77_max\",\n",
      "        \"D_77_last\",\n",
      "        \"B_25_first\",\n",
      "        \"B_25_mean\",\n",
      "        \"B_25_std\",\n",
      "        \"B_25_min\",\n",
      "        \"B_25_max\",\n",
      "        \"B_25_last\",\n",
      "        \"B_26_first\",\n",
      "        \"B_26_mean\",\n",
      "        \"B_26_std\",\n",
      "        \"B_26_min\",\n",
      "        \"B_26_max\",\n",
      "        \"B_26_last\",\n",
      "        \"D_78_first\",\n",
      "        \"D_78_mean\",\n",
      "        \"D_78_std\",\n",
      "        \"D_78_min\",\n",
      "        \"D_78_max\",\n",
      "        \"D_78_last\",\n",
      "        \"D_79_first\",\n",
      "        \"D_79_mean\",\n",
      "        \"D_79_std\",\n",
      "        \"D_79_min\",\n",
      "        \"D_79_max\",\n",
      "        \"D_79_last\",\n",
      "        \"R_8_first\",\n",
      "        \"R_8_mean\",\n",
      "        \"R_8_std\",\n",
      "        \"R_8_min\",\n",
      "        \"R_8_max\",\n",
      "        \"R_8_last\",\n",
      "        \"R_9_first\",\n",
      "        \"R_9_mean\",\n",
      "        \"R_9_std\",\n",
      "        \"R_9_min\",\n",
      "        \"R_9_max\",\n",
      "        \"R_9_last\",\n",
      "        \"S_16_first\",\n",
      "        \"S_16_mean\",\n",
      "        \"S_16_std\",\n",
      "        \"S_16_min\",\n",
      "        \"S_16_max\",\n",
      "        \"S_16_last\",\n",
      "        \"D_80_first\",\n",
      "        \"D_80_mean\",\n",
      "        \"D_80_std\",\n",
      "        \"D_80_min\",\n",
      "        \"D_80_max\",\n",
      "        \"D_80_last\",\n",
      "        \"R_10_first\",\n",
      "        \"R_10_mean\",\n",
      "        \"R_10_std\",\n",
      "        \"R_10_min\",\n",
      "        \"R_10_max\",\n",
      "        \"R_10_last\",\n",
      "        \"R_11_first\",\n",
      "        \"R_11_mean\",\n",
      "        \"R_11_std\",\n",
      "        \"R_11_min\",\n",
      "        \"R_11_max\",\n",
      "        \"R_11_last\",\n",
      "        \"B_27_std\",\n",
      "        \"B_27_min\",\n",
      "        \"B_27_max\",\n",
      "        \"B_27_last\",\n",
      "        \"D_81_first\",\n",
      "        \"D_81_mean\",\n",
      "        \"D_81_std\",\n",
      "        \"D_81_max\",\n",
      "        \"D_81_last\",\n",
      "        \"D_82_first\",\n",
      "        \"D_82_mean\",\n",
      "        \"D_82_std\",\n",
      "        \"D_82_min\",\n",
      "        \"D_82_max\",\n",
      "        \"D_82_last\",\n",
      "        \"S_17_first\",\n",
      "        \"S_17_mean\",\n",
      "        \"S_17_std\",\n",
      "        \"S_17_min\",\n",
      "        \"S_17_max\",\n",
      "        \"S_17_last\",\n",
      "        \"R_12_first\",\n",
      "        \"R_12_mean\",\n",
      "        \"R_12_std\",\n",
      "        \"R_12_min\",\n",
      "        \"R_12_max\",\n",
      "        \"R_12_last\",\n",
      "        \"B_28_first\",\n",
      "        \"B_28_mean\",\n",
      "        \"B_28_std\",\n",
      "        \"B_28_min\",\n",
      "        \"B_28_max\",\n",
      "        \"B_28_last\",\n",
      "        \"R_13_first\",\n",
      "        \"R_13_mean\",\n",
      "        \"R_13_std\",\n",
      "        \"R_13_min\",\n",
      "        \"R_13_max\",\n",
      "        \"R_13_last\",\n",
      "        \"D_83_first\",\n",
      "        \"D_83_mean\",\n",
      "        \"D_83_std\",\n",
      "        \"D_83_max\",\n",
      "        \"D_83_last\",\n",
      "        \"R_14_first\",\n",
      "        \"R_14_mean\",\n",
      "        \"R_14_std\",\n",
      "        \"R_14_min\",\n",
      "        \"R_14_max\",\n",
      "        \"R_14_last\",\n",
      "        \"R_15_mean\",\n",
      "        \"R_15_std\",\n",
      "        \"D_84_first\",\n",
      "        \"D_84_mean\",\n",
      "        \"D_84_std\",\n",
      "        \"D_84_min\",\n",
      "        \"D_84_max\",\n",
      "        \"D_84_last\",\n",
      "        \"R_16_first\",\n",
      "        \"R_16_mean\",\n",
      "        \"R_16_std\",\n",
      "        \"R_16_min\",\n",
      "        \"R_16_max\",\n",
      "        \"R_16_last\",\n",
      "        \"B_29_first\",\n",
      "        \"B_29_mean\",\n",
      "        \"B_29_std\",\n",
      "        \"B_29_min\",\n",
      "        \"B_29_max\",\n",
      "        \"B_29_last\",\n",
      "        \"S_18_mean\",\n",
      "        \"S_18_std\",\n",
      "        \"D_86_mean\",\n",
      "        \"D_86_std\",\n",
      "        \"D_87_mean\",\n",
      "        \"D_87_std\",\n",
      "        \"R_17_first\",\n",
      "        \"R_17_mean\",\n",
      "        \"R_17_std\",\n",
      "        \"R_17_min\",\n",
      "        \"R_17_max\",\n",
      "        \"R_17_last\",\n",
      "        \"R_18_first\",\n",
      "        \"R_18_mean\",\n",
      "        \"R_18_std\",\n",
      "        \"R_18_min\",\n",
      "        \"R_18_max\",\n",
      "        \"R_18_last\",\n",
      "        \"D_88_first\",\n",
      "        \"D_88_mean\",\n",
      "        \"D_88_std\",\n",
      "        \"D_88_min\",\n",
      "        \"D_88_max\",\n",
      "        \"D_88_last\",\n",
      "        \"B_31_mean\",\n",
      "        \"B_31_std\",\n",
      "        \"S_19_first\",\n",
      "        \"S_19_mean\",\n",
      "        \"S_19_std\",\n",
      "        \"S_19_min\",\n",
      "        \"S_19_max\",\n",
      "        \"R_19_mean\",\n",
      "        \"R_19_std\",\n",
      "        \"B_32_mean\",\n",
      "        \"B_32_std\",\n",
      "        \"S_20_mean\",\n",
      "        \"S_20_std\",\n",
      "        \"R_20_first\",\n",
      "        \"R_20_mean\",\n",
      "        \"R_20_std\",\n",
      "        \"R_20_min\",\n",
      "        \"R_20_max\",\n",
      "        \"R_20_last\",\n",
      "        \"R_21_mean\",\n",
      "        \"R_21_std\",\n",
      "        \"B_33_max\",\n",
      "        \"D_89_first\",\n",
      "        \"D_89_mean\",\n",
      "        \"D_89_std\",\n",
      "        \"D_89_min\",\n",
      "        \"D_89_max\",\n",
      "        \"D_89_last\",\n",
      "        \"R_22_mean\",\n",
      "        \"R_22_std\",\n",
      "        \"R_23_mean\",\n",
      "        \"R_23_std\",\n",
      "        \"D_91_mean\",\n",
      "        \"D_91_std\",\n",
      "        \"D_91_max\",\n",
      "        \"D_91_last\",\n",
      "        \"D_92_first\",\n",
      "        \"D_92_mean\",\n",
      "        \"D_92_std\",\n",
      "        \"D_92_min\",\n",
      "        \"D_92_max\",\n",
      "        \"D_92_last\",\n",
      "        \"D_93_mean\",\n",
      "        \"D_93_std\",\n",
      "        \"D_94_mean\",\n",
      "        \"D_94_std\",\n",
      "        \"R_24_mean\",\n",
      "        \"R_24_std\",\n",
      "        \"R_25_mean\",\n",
      "        \"R_25_std\",\n",
      "        \"D_96_mean\",\n",
      "        \"D_96_std\",\n",
      "        \"S_22_first\",\n",
      "        \"S_22_mean\",\n",
      "        \"S_22_std\",\n",
      "        \"S_22_min\",\n",
      "        \"S_22_max\",\n",
      "        \"S_22_last\",\n",
      "        \"S_23_first\",\n",
      "        \"S_23_mean\",\n",
      "        \"S_23_std\",\n",
      "        \"S_23_min\",\n",
      "        \"S_23_max\",\n",
      "        \"S_23_last\",\n",
      "        \"S_24_first\",\n",
      "        \"S_24_mean\",\n",
      "        \"S_24_std\",\n",
      "        \"S_24_min\",\n",
      "        \"S_24_max\",\n",
      "        \"S_24_last\",\n",
      "        \"S_25_first\",\n",
      "        \"S_25_mean\",\n",
      "        \"S_25_std\",\n",
      "        \"S_25_min\",\n",
      "        \"S_25_max\",\n",
      "        \"S_25_last\",\n",
      "        \"S_26_first\",\n",
      "        \"S_26_mean\",\n",
      "        \"S_26_std\",\n",
      "        \"S_26_min\",\n",
      "        \"S_26_max\",\n",
      "        \"S_26_last\",\n",
      "        \"D_102_first\",\n",
      "        \"D_102_mean\",\n",
      "        \"D_102_std\",\n",
      "        \"D_102_min\",\n",
      "        \"D_102_max\",\n",
      "        \"D_102_last\",\n",
      "        \"D_103_std\",\n",
      "        \"D_104_std\",\n",
      "        \"D_105_std\",\n",
      "        \"D_105_min\",\n",
      "        \"D_106_first\",\n",
      "        \"D_106_mean\",\n",
      "        \"D_106_std\",\n",
      "        \"D_106_min\",\n",
      "        \"D_106_max\",\n",
      "        \"D_106_last\",\n",
      "        \"D_107_mean\",\n",
      "        \"D_107_std\",\n",
      "        \"D_107_max\",\n",
      "        \"D_107_last\",\n",
      "        \"B_36_first\",\n",
      "        \"B_36_mean\",\n",
      "        \"B_36_std\",\n",
      "        \"B_36_min\",\n",
      "        \"B_36_max\",\n",
      "        \"B_36_last\",\n",
      "        \"B_37_first\",\n",
      "        \"B_37_mean\",\n",
      "        \"B_37_std\",\n",
      "        \"B_37_min\",\n",
      "        \"B_37_max\",\n",
      "        \"B_37_last\",\n",
      "        \"R_26_first\",\n",
      "        \"R_26_mean\",\n",
      "        \"R_26_std\",\n",
      "        \"R_26_min\",\n",
      "        \"R_26_max\",\n",
      "        \"R_26_last\",\n",
      "        \"R_27_first\",\n",
      "        \"R_27_mean\",\n",
      "        \"R_27_std\",\n",
      "        \"R_27_min\",\n",
      "        \"R_27_max\",\n",
      "        \"R_27_last\",\n",
      "        \"D_108_first\",\n",
      "        \"D_108_mean\",\n",
      "        \"D_108_std\",\n",
      "        \"D_108_max\",\n",
      "        \"D_108_last\",\n",
      "        \"D_109_mean\",\n",
      "        \"D_109_std\",\n",
      "        \"D_109_min\",\n",
      "        \"D_109_max\",\n",
      "        \"D_109_last\",\n",
      "        \"D_110_std\",\n",
      "        \"D_110_max\",\n",
      "        \"D_111_first\",\n",
      "        \"D_111_mean\",\n",
      "        \"D_111_std\",\n",
      "        \"D_111_min\",\n",
      "        \"D_111_max\",\n",
      "        \"D_111_last\",\n",
      "        \"B_39_first\",\n",
      "        \"B_39_mean\",\n",
      "        \"B_39_std\",\n",
      "        \"B_39_min\",\n",
      "        \"B_39_last\",\n",
      "        \"D_112_first\",\n",
      "        \"D_112_mean\",\n",
      "        \"D_112_std\",\n",
      "        \"D_112_max\",\n",
      "        \"D_112_last\",\n",
      "        \"B_40_first\",\n",
      "        \"B_40_mean\",\n",
      "        \"B_40_std\",\n",
      "        \"B_40_min\",\n",
      "        \"B_40_max\",\n",
      "        \"B_40_last\",\n",
      "        \"S_27_first\",\n",
      "        \"S_27_mean\",\n",
      "        \"S_27_std\",\n",
      "        \"S_27_min\",\n",
      "        \"S_27_max\",\n",
      "        \"S_27_last\",\n",
      "        \"D_113_first\",\n",
      "        \"D_113_mean\",\n",
      "        \"D_113_std\",\n",
      "        \"D_113_min\",\n",
      "        \"D_113_max\",\n",
      "        \"D_113_last\",\n",
      "        \"D_115_first\",\n",
      "        \"D_115_mean\",\n",
      "        \"D_115_std\",\n",
      "        \"D_115_min\",\n",
      "        \"D_115_max\",\n",
      "        \"D_115_last\",\n",
      "        \"D_118_first\",\n",
      "        \"D_118_mean\",\n",
      "        \"D_118_std\",\n",
      "        \"D_118_min\",\n",
      "        \"D_118_last\",\n",
      "        \"D_119_first\",\n",
      "        \"D_119_mean\",\n",
      "        \"D_119_std\",\n",
      "        \"D_119_min\",\n",
      "        \"D_119_last\",\n",
      "        \"D_121_std\",\n",
      "        \"D_122_std\",\n",
      "        \"D_123_mean\",\n",
      "        \"D_123_std\",\n",
      "        \"D_123_min\",\n",
      "        \"D_123_max\",\n",
      "        \"D_123_last\",\n",
      "        \"D_124_mean\",\n",
      "        \"D_124_std\",\n",
      "        \"D_124_max\",\n",
      "        \"D_124_last\",\n",
      "        \"D_125_std\",\n",
      "        \"D_125_min\",\n",
      "        \"D_125_max\",\n",
      "        \"D_125_last\",\n",
      "        \"D_127_mean\",\n",
      "        \"D_127_std\",\n",
      "        \"D_128_std\",\n",
      "        \"D_129_std\",\n",
      "        \"B_41_first\",\n",
      "        \"B_41_mean\",\n",
      "        \"B_41_std\",\n",
      "        \"B_41_min\",\n",
      "        \"B_41_max\",\n",
      "        \"B_41_last\",\n",
      "        \"B_42_first\",\n",
      "        \"B_42_mean\",\n",
      "        \"B_42_std\",\n",
      "        \"B_42_min\",\n",
      "        \"B_42_max\",\n",
      "        \"B_42_last\",\n",
      "        \"D_130_first\",\n",
      "        \"D_130_mean\",\n",
      "        \"D_130_std\",\n",
      "        \"D_130_min\",\n",
      "        \"D_130_max\",\n",
      "        \"D_130_last\",\n",
      "        \"D_131_first\",\n",
      "        \"D_131_mean\",\n",
      "        \"D_131_std\",\n",
      "        \"D_131_min\",\n",
      "        \"D_131_max\",\n",
      "        \"D_131_last\",\n",
      "        \"D_132_first\",\n",
      "        \"D_132_mean\",\n",
      "        \"D_132_std\",\n",
      "        \"D_132_min\",\n",
      "        \"D_132_max\",\n",
      "        \"D_132_last\",\n",
      "        \"D_133_first\",\n",
      "        \"D_133_mean\",\n",
      "        \"D_133_std\",\n",
      "        \"D_133_min\",\n",
      "        \"D_133_max\",\n",
      "        \"D_133_last\",\n",
      "        \"R_28_mean\",\n",
      "        \"R_28_std\",\n",
      "        \"D_134_first\",\n",
      "        \"D_134_mean\",\n",
      "        \"D_134_std\",\n",
      "        \"D_134_min\",\n",
      "        \"D_134_last\",\n",
      "        \"D_135_first\",\n",
      "        \"D_135_mean\",\n",
      "        \"D_135_std\",\n",
      "        \"D_135_max\",\n",
      "        \"D_135_last\",\n",
      "        \"D_136_first\",\n",
      "        \"D_136_mean\",\n",
      "        \"D_136_std\",\n",
      "        \"D_136_min\",\n",
      "        \"D_136_max\",\n",
      "        \"D_136_last\",\n",
      "        \"D_137_first\",\n",
      "        \"D_137_mean\",\n",
      "        \"D_137_std\",\n",
      "        \"D_137_min\",\n",
      "        \"D_137_max\",\n",
      "        \"D_137_last\",\n",
      "        \"D_138_first\",\n",
      "        \"D_138_mean\",\n",
      "        \"D_138_std\",\n",
      "        \"D_138_min\",\n",
      "        \"D_138_max\",\n",
      "        \"D_138_last\",\n",
      "        \"D_139_mean\",\n",
      "        \"D_139_std\",\n",
      "        \"D_139_max\",\n",
      "        \"D_139_last\",\n",
      "        \"D_140_first\",\n",
      "        \"D_140_mean\",\n",
      "        \"D_140_std\",\n",
      "        \"D_140_min\",\n",
      "        \"D_141_first\",\n",
      "        \"D_141_mean\",\n",
      "        \"D_141_std\",\n",
      "        \"D_141_min\",\n",
      "        \"D_141_max\",\n",
      "        \"D_141_last\",\n",
      "        \"D_142_std\",\n",
      "        \"D_143_mean\",\n",
      "        \"D_143_std\",\n",
      "        \"D_143_max\",\n",
      "        \"D_143_last\",\n",
      "        \"D_144_first\",\n",
      "        \"D_144_mean\",\n",
      "        \"D_144_std\",\n",
      "        \"D_144_min\",\n",
      "        \"D_144_max\",\n",
      "        \"D_144_last\",\n",
      "        \"D_145_first\",\n",
      "        \"D_145_mean\",\n",
      "        \"D_145_std\",\n",
      "        \"D_145_min\",\n",
      "        \"D_145_max\",\n",
      "        \"D_145_last\",\n",
      "        \"P_2_last_lag_sub\",\n",
      "        \"P_2_last_lag_div\",\n",
      "        \"D_39_last_lag_sub\",\n",
      "        \"B_1_last_lag_sub\",\n",
      "        \"B_1_last_lag_div\",\n",
      "        \"B_2_last_lag_div\",\n",
      "        \"R_1_last_lag_sub\",\n",
      "        \"R_1_last_lag_div\",\n",
      "        \"S_3_last_lag_div\",\n",
      "        \"D_41_last_lag_sub\",\n",
      "        \"B_3_last_lag_sub\",\n",
      "        \"B_3_last_lag_div\",\n",
      "        \"D_42_last_lag_sub\",\n",
      "        \"D_42_last_lag_div\",\n",
      "        \"D_43_last_lag_sub\",\n",
      "        \"D_43_last_lag_div\",\n",
      "        \"D_44_last_lag_sub\",\n",
      "        \"B_4_last_lag_sub\",\n",
      "        \"D_45_last_lag_sub\",\n",
      "        \"D_45_last_lag_div\",\n",
      "        \"B_5_last_lag_sub\",\n",
      "        \"B_5_last_lag_div\",\n",
      "        \"R_2_last_lag_sub\",\n",
      "        \"D_46_last_lag_div\",\n",
      "        \"D_47_last_lag_div\",\n",
      "        \"D_48_last_lag_div\",\n",
      "        \"D_49_last_lag_sub\",\n",
      "        \"B_6_last_lag_sub\",\n",
      "        \"B_6_last_lag_div\",\n",
      "        \"B_7_last_lag_div\",\n",
      "        \"D_50_last_lag_sub\",\n",
      "        \"D_50_last_lag_div\",\n",
      "        \"B_9_last_lag_sub\",\n",
      "        \"B_9_last_lag_div\",\n",
      "        \"R_3_last_lag_sub\",\n",
      "        \"D_52_last_lag_div\",\n",
      "        \"P_3_last_lag_div\",\n",
      "        \"B_10_last_lag_sub\",\n",
      "        \"B_10_last_lag_div\",\n",
      "        \"D_53_last_lag_sub\",\n",
      "        \"D_53_last_lag_div\",\n",
      "        \"S_5_last_lag_sub\",\n",
      "        \"S_5_last_lag_div\",\n",
      "        \"B_11_last_lag_sub\",\n",
      "        \"B_11_last_lag_div\",\n",
      "        \"D_54_last_lag_sub\",\n",
      "        \"D_54_last_lag_div\",\n",
      "        \"R_4_last_lag_sub\",\n",
      "        \"S_7_last_lag_div\",\n",
      "        \"B_12_last_lag_div\",\n",
      "        \"D_55_last_lag_div\",\n",
      "        \"D_56_last_lag_div\",\n",
      "        \"B_13_last_lag_sub\",\n",
      "        \"B_13_last_lag_div\",\n",
      "        \"R_5_last_lag_sub\",\n",
      "        \"D_58_last_lag_div\",\n",
      "        \"S_9_last_lag_sub\",\n",
      "        \"S_9_last_lag_div\",\n",
      "        \"B_14_last_lag_sub\",\n",
      "        \"B_14_last_lag_div\",\n",
      "        \"D_59_last_lag_sub\",\n",
      "        \"D_59_last_lag_div\",\n",
      "        \"D_60_last_lag_div\",\n",
      "        \"D_61_last_lag_sub\",\n",
      "        \"D_61_last_lag_div\",\n",
      "        \"B_15_last_lag_sub\",\n",
      "        \"B_15_last_lag_div\",\n",
      "        \"D_62_last_lag_sub\",\n",
      "        \"D_62_last_lag_div\",\n",
      "        \"D_65_last_lag_sub\",\n",
      "        \"B_17_last_lag_div\",\n",
      "        \"B_18_last_lag_div\",\n",
      "        \"B_19_last_lag_sub\",\n",
      "        \"S_12_last_lag_sub\",\n",
      "        \"S_12_last_lag_div\",\n",
      "        \"R_6_last_lag_sub\",\n",
      "        \"R_6_last_lag_div\",\n",
      "        \"B_21_last_lag_sub\",\n",
      "        \"B_21_last_lag_div\",\n",
      "        \"D_69_last_lag_sub\",\n",
      "        \"D_69_last_lag_div\",\n",
      "        \"D_70_last_lag_sub\",\n",
      "        \"D_71_last_lag_sub\",\n",
      "        \"D_71_last_lag_div\",\n",
      "        \"D_72_last_lag_sub\",\n",
      "        \"B_23_last_lag_div\",\n",
      "        \"D_73_last_lag_sub\",\n",
      "        \"D_73_last_lag_div\",\n",
      "        \"P_4_last_lag_sub\",\n",
      "        \"D_74_last_lag_sub\",\n",
      "        \"D_75_last_lag_sub\",\n",
      "        \"D_76_last_lag_sub\",\n",
      "        \"D_76_last_lag_div\",\n",
      "        \"B_24_last_lag_sub\",\n",
      "        \"B_24_last_lag_div\",\n",
      "        \"R_7_last_lag_sub\",\n",
      "        \"D_77_last_lag_sub\",\n",
      "        \"D_77_last_lag_div\",\n",
      "        \"B_25_last_lag_sub\",\n",
      "        \"B_25_last_lag_div\",\n",
      "        \"B_26_last_lag_sub\",\n",
      "        \"B_26_last_lag_div\",\n",
      "        \"D_78_last_lag_sub\",\n",
      "        \"D_79_last_lag_sub\",\n",
      "        \"R_8_last_lag_sub\",\n",
      "        \"R_9_last_lag_sub\",\n",
      "        \"S_16_last_lag_sub\",\n",
      "        \"S_16_last_lag_div\",\n",
      "        \"R_10_last_lag_sub\",\n",
      "        \"B_27_last_lag_div\",\n",
      "        \"D_81_last_lag_sub\",\n",
      "        \"D_82_last_lag_sub\",\n",
      "        \"S_17_last_lag_div\",\n",
      "        \"R_12_last_lag_sub\",\n",
      "        \"R_12_last_lag_div\",\n",
      "        \"B_28_last_lag_sub\",\n",
      "        \"B_28_last_lag_div\",\n",
      "        \"R_13_last_lag_sub\",\n",
      "        \"R_14_last_lag_sub\",\n",
      "        \"R_15_last_lag_sub\",\n",
      "        \"D_84_last_lag_sub\",\n",
      "        \"R_16_last_lag_sub\",\n",
      "        \"B_29_last_lag_sub\",\n",
      "        \"B_29_last_lag_div\",\n",
      "        \"S_18_last_lag_sub\",\n",
      "        \"D_87_last_lag_sub\",\n",
      "        \"R_17_last_lag_sub\",\n",
      "        \"R_18_last_lag_sub\",\n",
      "        \"D_88_last_lag_sub\",\n",
      "        \"D_88_last_lag_div\",\n",
      "        \"B_31_last_lag_sub\",\n",
      "        \"S_19_last_lag_div\",\n",
      "        \"R_19_last_lag_sub\",\n",
      "        \"B_32_last_lag_sub\",\n",
      "        \"R_20_last_lag_sub\",\n",
      "        \"D_89_last_lag_sub\",\n",
      "        \"R_22_last_lag_sub\",\n",
      "        \"R_23_last_lag_sub\",\n",
      "        \"D_93_last_lag_sub\",\n",
      "        \"R_24_last_lag_sub\",\n",
      "        \"R_25_last_lag_sub\",\n",
      "        \"S_22_last_lag_sub\",\n",
      "        \"S_22_last_lag_div\",\n",
      "        \"S_23_last_lag_sub\",\n",
      "        \"S_23_last_lag_div\",\n",
      "        \"S_24_last_lag_sub\",\n",
      "        \"S_24_last_lag_div\",\n",
      "        \"S_25_last_lag_sub\",\n",
      "        \"S_25_last_lag_div\",\n",
      "        \"S_26_last_lag_sub\",\n",
      "        \"S_26_last_lag_div\",\n",
      "        \"D_102_last_lag_sub\",\n",
      "        \"D_102_last_lag_div\",\n",
      "        \"D_103_last_lag_sub\",\n",
      "        \"D_105_last_lag_sub\",\n",
      "        \"D_105_last_lag_div\",\n",
      "        \"D_107_last_lag_sub\",\n",
      "        \"B_36_last_lag_sub\",\n",
      "        \"B_36_last_lag_div\",\n",
      "        \"B_37_last_lag_sub\",\n",
      "        \"B_37_last_lag_div\",\n",
      "        \"R_26_last_lag_sub\",\n",
      "        \"R_27_last_lag_sub\",\n",
      "        \"R_27_last_lag_div\",\n",
      "        \"D_108_last_lag_sub\",\n",
      "        \"D_109_last_lag_sub\",\n",
      "        \"D_110_last_lag_sub\",\n",
      "        \"D_110_last_lag_div\",\n",
      "        \"D_111_last_lag_sub\",\n",
      "        \"B_39_last_lag_div\",\n",
      "        \"D_112_last_lag_div\",\n",
      "        \"B_40_last_lag_sub\",\n",
      "        \"B_40_last_lag_div\",\n",
      "        \"S_27_last_lag_sub\",\n",
      "        \"S_27_last_lag_div\",\n",
      "        \"D_113_last_lag_sub\",\n",
      "        \"D_115_last_lag_div\",\n",
      "        \"D_118_last_lag_div\",\n",
      "        \"D_119_last_lag_div\",\n",
      "        \"D_121_last_lag_sub\",\n",
      "        \"D_121_last_lag_div\",\n",
      "        \"D_122_last_lag_sub\",\n",
      "        \"D_123_last_lag_sub\",\n",
      "        \"D_124_last_lag_sub\",\n",
      "        \"D_128_last_lag_sub\",\n",
      "        \"D_129_last_lag_sub\",\n",
      "        \"B_41_last_lag_sub\",\n",
      "        \"B_42_last_lag_sub\",\n",
      "        \"B_42_last_lag_div\",\n",
      "        \"D_130_last_lag_sub\",\n",
      "        \"D_131_last_lag_sub\",\n",
      "        \"D_132_last_lag_sub\",\n",
      "        \"D_132_last_lag_div\",\n",
      "        \"D_133_last_lag_sub\",\n",
      "        \"D_133_last_lag_div\",\n",
      "        \"R_28_last_lag_sub\",\n",
      "        \"D_134_last_lag_sub\",\n",
      "        \"D_134_last_lag_div\",\n",
      "        \"D_136_last_lag_sub\",\n",
      "        \"D_139_last_lag_sub\",\n",
      "        \"D_140_last_lag_sub\",\n",
      "        \"D_141_last_lag_sub\",\n",
      "        \"D_142_last_lag_sub\",\n",
      "        \"D_142_last_lag_div\",\n",
      "        \"D_143_last_lag_sub\",\n",
      "        \"D_144_last_lag_sub\",\n",
      "        \"D_144_last_lag_div\",\n",
      "        \"D_145_last_lag_sub\",\n",
      "        \"B_30_count\",\n",
      "        \"B_30_first\",\n",
      "        \"B_30_last\",\n",
      "        \"B_30_nunique\",\n",
      "        \"B_38_count\",\n",
      "        \"B_38_first\",\n",
      "        \"B_38_last\",\n",
      "        \"D_114_count\",\n",
      "        \"D_114_nunique\",\n",
      "        \"D_116_count\",\n",
      "        \"D_116_first\",\n",
      "        \"D_116_last\",\n",
      "        \"D_116_nunique\",\n",
      "        \"D_117_count\",\n",
      "        \"D_120_count\",\n",
      "        \"D_120_last\",\n",
      "        \"D_120_nunique\",\n",
      "        \"D_126_count\",\n",
      "        \"D_126_first\",\n",
      "        \"D_126_nunique\",\n",
      "        \"D_63_count\",\n",
      "        \"D_63_first\",\n",
      "        \"D_63_last\",\n",
      "        \"D_63_nunique\",\n",
      "        \"D_64_count\",\n",
      "        \"D_64_nunique\",\n",
      "        \"D_66_count\",\n",
      "        \"D_66_nunique\",\n",
      "        \"D_68_count\",\n",
      "        \"D_68_first\",\n",
      "        \"D_68_last\",\n",
      "        \"D_68_nunique\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"R_2_first\",\n",
      "        \"R_2_min\",\n",
      "        \"R_2_max\",\n",
      "        \"R_2_last\",\n",
      "        \"S_6_first\",\n",
      "        \"S_6_min\",\n",
      "        \"S_6_max\",\n",
      "        \"S_6_last\",\n",
      "        \"R_4_first\",\n",
      "        \"R_4_min\",\n",
      "        \"R_4_max\",\n",
      "        \"R_4_last\",\n",
      "        \"R_15_first\",\n",
      "        \"R_15_min\",\n",
      "        \"R_15_max\",\n",
      "        \"R_15_last\",\n",
      "        \"S_18_first\",\n",
      "        \"S_18_min\",\n",
      "        \"S_18_max\",\n",
      "        \"S_18_last\",\n",
      "        \"D_86_first\",\n",
      "        \"D_86_min\",\n",
      "        \"D_86_max\",\n",
      "        \"D_86_last\",\n",
      "        \"D_87_first\",\n",
      "        \"D_87_min\",\n",
      "        \"D_87_max\",\n",
      "        \"D_87_last\",\n",
      "        \"B_31_first\",\n",
      "        \"B_31_min\",\n",
      "        \"B_31_max\",\n",
      "        \"B_31_last\",\n",
      "        \"R_19_first\",\n",
      "        \"R_19_min\",\n",
      "        \"R_19_max\",\n",
      "        \"R_19_last\",\n",
      "        \"B_32_first\",\n",
      "        \"B_32_min\",\n",
      "        \"B_32_max\",\n",
      "        \"B_32_last\",\n",
      "        \"S_20_first\",\n",
      "        \"S_20_min\",\n",
      "        \"S_20_max\",\n",
      "        \"S_20_last\",\n",
      "        \"R_21_first\",\n",
      "        \"R_21_min\",\n",
      "        \"R_21_max\",\n",
      "        \"R_21_last\",\n",
      "        \"R_22_first\",\n",
      "        \"R_22_min\",\n",
      "        \"R_22_max\",\n",
      "        \"R_22_last\",\n",
      "        \"R_23_first\",\n",
      "        \"R_23_max\",\n",
      "        \"R_23_last\",\n",
      "        \"D_93_first\",\n",
      "        \"D_93_min\",\n",
      "        \"D_93_max\",\n",
      "        \"D_93_last\",\n",
      "        \"D_94_first\",\n",
      "        \"D_94_min\",\n",
      "        \"D_94_max\",\n",
      "        \"D_94_last\",\n",
      "        \"R_24_first\",\n",
      "        \"R_24_min\",\n",
      "        \"R_24_max\",\n",
      "        \"R_24_last\",\n",
      "        \"R_25_first\",\n",
      "        \"R_25_min\",\n",
      "        \"R_25_max\",\n",
      "        \"R_25_last\",\n",
      "        \"D_96_first\",\n",
      "        \"D_96_min\",\n",
      "        \"D_96_max\",\n",
      "        \"D_96_last\",\n",
      "        \"D_108_min\",\n",
      "        \"D_127_first\",\n",
      "        \"D_127_min\",\n",
      "        \"D_127_max\",\n",
      "        \"D_127_last\",\n",
      "        \"R_28_first\",\n",
      "        \"R_28_min\",\n",
      "        \"R_28_max\",\n",
      "        \"R_28_last\",\n",
      "        \"D_135_min\",\n",
      "        \"D_140_max\",\n",
      "        \"D_140_last\",\n",
      "        \"D_87_last_lag_div\",\n",
      "        \"D_126_last\",\n",
      "        \"D_66_first\",\n",
      "        \"D_66_last\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput contains infinity or a value too large for dtype('float64').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 217, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 222, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 252, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **kwargs_fold)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 180, in _fit\n",
      "    train_dataset, val_dataset = self._generate_datasets(X=X, y=y, params=processor_kwargs, X_val=X_val, y_val=y_val)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 451, in _generate_datasets\n",
      "    train_dataset = self._process_train_data(df=X, labels=y,\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 523, in _process_train_data\n",
      "    df = self.processor.fit_transform(df)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 675, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 606, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\Kaleb\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Kaleb\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Kaleb\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Kaleb\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Kaleb\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Kaleb\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\pipeline.py\", line 426, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\Kaleb\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 852, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\impute\\_base.py\", line 319, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\impute\\_base.py\", line 287, in _validate_input\n",
      "    raise ve\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\impute\\_base.py\", line 270, in _validate_input\n",
      "    X = self._validate_data(\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 566, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"C:\\Users\\Kaleb\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains infinity or a value too large for dtype('float64').\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2288.2s of the 2288.18s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.284228\n",
      "[100]\tvalid_set's binary_logloss: 0.245046\n",
      "[150]\tvalid_set's binary_logloss: 0.236688\n",
      "[200]\tvalid_set's binary_logloss: 0.234647\n",
      "[250]\tvalid_set's binary_logloss: 0.234112\n",
      "[300]\tvalid_set's binary_logloss: 0.234346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.282437\n",
      "[100]\tvalid_set's binary_logloss: 0.240567\n",
      "[150]\tvalid_set's binary_logloss: 0.231967\n",
      "[200]\tvalid_set's binary_logloss: 0.229594\n",
      "[250]\tvalid_set's binary_logloss: 0.229035\n",
      "[300]\tvalid_set's binary_logloss: 0.229157\n",
      "[350]\tvalid_set's binary_logloss: 0.229582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.280416\n",
      "[100]\tvalid_set's binary_logloss: 0.238965\n",
      "[150]\tvalid_set's binary_logloss: 0.229396\n",
      "[200]\tvalid_set's binary_logloss: 0.226942\n",
      "[250]\tvalid_set's binary_logloss: 0.226414\n",
      "[300]\tvalid_set's binary_logloss: 0.226487\n",
      "[350]\tvalid_set's binary_logloss: 0.227098\n",
      "[400]\tvalid_set's binary_logloss: 0.228002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.278124\n",
      "[100]\tvalid_set's binary_logloss: 0.23558\n",
      "[150]\tvalid_set's binary_logloss: 0.22594\n",
      "[200]\tvalid_set's binary_logloss: 0.223142\n",
      "[250]\tvalid_set's binary_logloss: 0.222066\n",
      "[300]\tvalid_set's binary_logloss: 0.221825\n",
      "[350]\tvalid_set's binary_logloss: 0.222009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.283391\n",
      "[100]\tvalid_set's binary_logloss: 0.242749\n",
      "[150]\tvalid_set's binary_logloss: 0.233278\n",
      "[200]\tvalid_set's binary_logloss: 0.230153\n",
      "[250]\tvalid_set's binary_logloss: 0.229365\n",
      "[300]\tvalid_set's binary_logloss: 0.229398\n",
      "[350]\tvalid_set's binary_logloss: 0.229557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.278055\n",
      "[100]\tvalid_set's binary_logloss: 0.234647\n",
      "[150]\tvalid_set's binary_logloss: 0.224604\n",
      "[200]\tvalid_set's binary_logloss: 0.22186\n",
      "[250]\tvalid_set's binary_logloss: 0.220358\n",
      "[300]\tvalid_set's binary_logloss: 0.220504\n",
      "[350]\tvalid_set's binary_logloss: 0.220516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.27611\n",
      "[100]\tvalid_set's binary_logloss: 0.233757\n",
      "[150]\tvalid_set's binary_logloss: 0.22396\n",
      "[200]\tvalid_set's binary_logloss: 0.220423\n",
      "[250]\tvalid_set's binary_logloss: 0.219289\n",
      "[300]\tvalid_set's binary_logloss: 0.218761\n",
      "[350]\tvalid_set's binary_logloss: 0.218651\n",
      "[400]\tvalid_set's binary_logloss: 0.219267\n",
      "[450]\tvalid_set's binary_logloss: 0.219596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 1, 'num_cpus': 6\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'device': 'gpu'}\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.280672\n",
      "[100]\tvalid_set's binary_logloss: 0.239687\n",
      "[150]\tvalid_set's binary_logloss: 0.230613\n",
      "[200]\tvalid_set's binary_logloss: 0.227773\n",
      "[250]\tvalid_set's binary_logloss: 0.226909\n",
      "[300]\tvalid_set's binary_logloss: 0.226799\n",
      "[350]\tvalid_set's binary_logloss: 0.227354\n",
      "[400]\tvalid_set's binary_logloss: 0.228079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-0.2257\t = Validation score   (-log_loss)\n",
      "\t939.46s\t = Training   runtime\n",
      "\t2.19s\t = Validation runtime\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1340.27s of remaining time.\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 66\n",
      "Ensemble weights: \n",
      "[0.34848485 0.10606061 0.39393939 0.15151515]\n",
      "\t0.46s\t= Estimated out-of-fold prediction time...\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\WeightedEnsemble_L2\\utils\\oof.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "\t-0.2218\t = Validation score   (-log_loss)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Saving E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "AutoGluon training complete, total runtime = 2262.91s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\trainer.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\utils\\data\\X.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\WeightedEnsemble_L2\\utils\\oof.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\utils\\data\\y.pkl\n",
      "Loading: E:/datasets/amex-default-prediction/models/ag_models/model_4\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Temperature scaling term being tuned for model: WeightedEnsemble_L2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20908/2956421774.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m predictor = TabularPredictor(label=label, eval_metric=eval_metric, \n\u001b[0m\u001b[0;32m      2\u001b[0m                              \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                 \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpresets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best_quality'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3600\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                 \u001b[0mag_args_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'num_gpus'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_stack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mgargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mother_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_inner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    837\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_post_fit_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m         self._post_fit(\n\u001b[0m\u001b[0;32m    840\u001b[0m             \u001b[0mkeep_only_best\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keep_only_best'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m             \u001b[0mrefit_full\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'refit_full'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py\u001b[0m in \u001b[0;36m_post_fit\u001b[1;34m(self, keep_only_best, refit_full, set_best_to_refit_full, save_space, calibrate, infer_limit)\u001b[0m\n\u001b[0;32m    890\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcalibrate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPROBLEM_TYPES_CLASSIFICATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calibrate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mQUANTILE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calibrate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py\u001b[0m in \u001b[0;36m_calibrate_model\u001b[1;34m(self, model_name, lr, max_iter, init_val)\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'Temperature scaling term being tuned for model: {model_name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m             temp_scalar = tune_temperature_scaling(y_val_probs=y_val_probs, y_val=y_val,\n\u001b[0m\u001b[0;32m    963\u001b[0m                                                    init_val=init_val, max_iter=max_iter, lr=lr)\n\u001b[0;32m    964\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtemp_scalar\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\calibrate\\temperature_scaling.py\u001b[0m in \u001b[0;36mtune_temperature_scaling\u001b[1;34m(y_val_probs, y_val, init_val, max_iter, lr)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemperature_scale_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mtemperature_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemperature_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0morig_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\autogluon\\core\\calibrate\\temperature_scaling.py\u001b[0m in \u001b[0;36mtemperature_scale_step\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemperature_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mnew_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnll_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3014\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label, eval_metric=eval_metric, \n",
    "                             path=save_path, verbosity=3).fit(\n",
    "                                train, presets='best_quality', time_limit=3600,\n",
    "                                ag_args_fit={'num_gpus':1}, auto_stack=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1dc8d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20908/3185282044.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "results=predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41fd50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
